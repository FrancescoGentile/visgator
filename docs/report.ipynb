{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referring Expression Comprehension as Scene Graph Grounding\n",
    "\n",
    "### Authors\n",
    "\n",
    "Diego Calanzone, Francesco Gentile <br>\n",
    "University of Trento <br>\n",
    "Deep Learning course project, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes on the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring Expression Comprehension (REC) is the task of localizing a target object in an image given a natural language expression that refers to it. Most recent approaches ([Zhang et al. 2022](https://arxiv.org/abs/2206.05836), [Xu et al. 2023](https://arxiv.org/abs/2302.00402), [Liu et al. 2023](https://arxiv.org/abs/2303.05499)) that obtain state-of-the-art results on this task are not specifically designed for it, but they are designed to solve a large variety of tasks that require fusing vision and language modalities, like open-set object detection, image captioning, visual question answering and so on. In particular, most of these first independently encode the the visual and textual input using vision and text encoders (based on the Transformer architecture) respectively, then another transformer module is used to fuse the two modalities by making the visual features attend to the textual features and vice versa. Finally, the fused features are given in input to another module (a simple head, a transformer decoder, etc.) based on the task that is being solved.\n",
    "\n",
    "Here we argue that the task of REC requires an high-level understanding of the scene described the region caption. For example, given a caption like *\"The girl approaching the table while holding a glass\"*, to correctly localize the associated bounding box, we need to first identify all the entities referred by the sentence (`the girl`, `the table`, `a glass`) and the relation that exist among them ((`the girl` -- `approaching` -> `the table`), (`the girl` -- `holding` --> `a glass`)). In other words, we need to extract from the sequence of words that form the sentence an intermediate higher-level representation of the scene. Then, instead of grounding the sequence of words to the image, we can ground the intermediate representation to the image. On the other hand, previously cited approaches, since they need to generalize to many image-text tasks, simply ground the word features (here we make the simplifying assumption that each token correspond to a word) extracted by the text encoder to the image features. Thus, to perform well in such task, the text encoder need to encode into each token not only the meaning of the corresponding word but also its relations with the other entities that in the sentence may be refered by group of tokens. In toher words, the text encoder need to learn to extract from the sequence of tokens a higher-level representation without being explicitly supervised to do so.\n",
    "\n",
    "Based on this observation, we propose a new approach to REC that is specifically designed for this task by making the network directly exploit the higher-level semantic information encoded in the input sentence. In particular, from the input sentence we extract a scene graph representing which entities are present in the region and how they are related to each other. Then, we localize the target region by localizing in the image the referred entities that satisfy the referred relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level architecture overview\n",
    "\n",
    "Our architecture is highly inspired to DETR-like models ([Carion et al. 2020](https://arxiv.org/abs/2005.12872), [Gao et al. 2021](https://arxiv.org/abs/2101.07448), [Liu et al. 2022](https://arxiv.org/abs/2201.12329)). Such models use a (CNN) backbone followed by a transformer-based vision encoder to extract visual features from the input image. Then a set of queries (representing candidate bounding boxes) is given in input to a transformer-based decoder, where such queries go through layers of self-attention and cross-attention with the visual features. Finally, the output of the decoder is given in input to a simple head that predicts the bounding boxes coordinates and the class of each bounding box. At training time, Hungarian matching is used to obtain a one-to-one matching between a query and a ground truth bounding box. Once such association is obtained, the loss is computed by comparing the predicted bounding box with the associated ground truth bounding box. At inference time, the predicted bounding boxes are filtered by a simple post-processing step to remove the predicted bounding boxes that have a low confidence score.\n",
    "\n",
    "Similarly, we extract the visual features by employing a transformer-based vision encoder (no backbone is used since we use the CLIP vision encoder). Then, differently from DETR-like models, we do not generate a predefined set of fixed or learnable queries, but we create a graph based on the one extracted from the sentence. In particular, for each entity in the sentence scene graph we create multiple nodes in the graph (since in the image there may be multiple instances of the same entity) whose embeddings are initialized with the embedding obtained by giving in input to the CLIP text encoder the entity textual description extracted from the sentence. Then, we create an edge between two nodes if the corresponding entities are related in the sentence scene graph; as before, the edge features are initialized by encoding the textual description of the relation with the CLIP text encoder. Then, the generated graph is given in input to the transformer-based decoder whose blocks consist of a sequence of `Multi-Head Cross Attention`, `Graph Attention` and `FFN`.\n",
    "\n",
    "In the multi-head cross attention layer, each query (nodes + edges) can attend to the visual features extracted from the vision encoder. This allow each query to verify whether the associated entity/relation is present in a specific region of the image. Then, in the graph attention layer, each node can communicate with its neighbours and the associated relations to verify whether the encoded instance of the entity satisfy the relations encoded in the sentence scene graph. \n",
    "\n",
    "Finally, from the graph outputted by the decoder, we extract the nodes that represent the subject of the sentence (i.e. the target entity) and we give them in input to a simple head to obtain candidate bounding boxes for the target entity. At training time, a simple matching algorithm is applied to associate the ground truth bounding box with one of the predicted bounding boxes and the loss is computed. At inference time, we select the node (and the obtained bounding box) whose embedding is the most similar to the embedding obtained by giving in input to the CLIP text encoder the full sentence.\n",
    "\n",
    "As currently presented, the decoder should also perform open-set object detection, since for each entity it should localize all the instances in the image. Thus, we should create a sufficient high number of nodes for each entity to be able to localize all the instances of the entity in the image. For example, Grounding DINO ([Liu et al. 2023](https://arxiv.org/abs/2303.05499)) creates 900 queries for each image. This would clearly require a lot of memory and computation power. Furthermore, current open-set object detectors are trained on huge amounts of data, on many GPUs and for long period of times (Grounding DINO uses 64 A100). Given the limited resources availables, we decided to employ an open-set object detector to obtain all instances of an entity in the image and an estimate of their location. In this way the decoder does not need to perform open-set object detection from scratch but it only needs to refine the estimated locations. Since existing open-set object detectors are mainly trained on closed object detection datasets, where each entity to be detected is represented by a single noun (i.e., the category name), to make the detector localize an entity we do not use its full textual description. Instead, for each entity we extract a single noun that best describe that entity. For example, given the entity `The woman with dark hair`, we extract the noun `woman` to localize the entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to extract scene graphs?\n",
    "\n",
    "As previously said, one of the first step is the extraction of the region scene graph from its text description. This task can be seen as the union of two closely related problems: named entity recognition and relation extraction. Since these tasks have long been studied by the NLP community, we have tried many existing solutions or we took inspiration from them to build our own. In the following we will describe the main approaches we have tried.\n",
    "\n",
    "Before diving into details, we notice that the generation of a scene graph from a sentence is an ambiguous task, that is the same sentence could be parsed into different scene graphs. When two noun phrases are connected by an action verb, it seems obvious to identify each noun as an entity and the verb as a relation. However, when the nouns are connected by a preposition, the situation is more ambiguous. For example, in the sentence *\"the woman in a green shirt\"*, the noun phrase `a green shirt` could be considered an attribute of `the woman` or a different entity related to `the woman` with the relation `in`/`wearing`. Similarly, in the sentence *\"the woman on the right\"*, some people may consider `the right` as an actual physical location and thus as an entity, while others may consider it as an attribute of `the woman`.\n",
    "\n",
    "Since most phrases in the RefCOCOg dataset are quite short, if we preferred the attribute interpretation, we would have obtained many scene graphs with very few nodes and edges or no edges at all, thus jeopardizing the idea underlying the model. For this reason, we have generally preferred the creation of a new entity for each noun phrase. However, we have preferred the attribute interpretation, when we thought that the detector would find it difficult to localize such entity (for example, in the case of spatial locations like `the right`, `the left`, `the background`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Graph based parsers\n",
    "\n",
    "Historically, one of the first approach to parse sentences in natural language into scene graphs was the one proposed by ([Schuster et al. 2015](https://aclanthology.org/W15-2812.pdf)). First the sentence is parsed into a semantic graph (i.e., a dependency graph to which some refinements are applied, like the handling of pronouns and plural nouns) using the CoreNLP pipeline. Then, based on a set of human-written rules, the semantic graph is converted into a scene graph. This approach has been used in many past works for different purposes, like evaluating generated image captions ([Anderson et al. 2016](https://arxiv.org/abs/1607.08822)) or creating pseudo ground truth scene graphs for Weakly Supervised Scene Graph Generation ([Ye et al. 2021](https://arxiv.org/abs/2105.13994), [Zhong et al. 2021](https://arxiv.org/abs/2109.02227), [Li et al. 2022](https://arxiv.org/abs/2208.01834)). \n",
    "\n",
    "Note that, since the original parser is written in Java, we used a Python-based [tool](https://github.com/vacancy/SceneGraphParser) that covers all the rules implemented in the Stanford Parser with some additional ones (however, it does not implement some features like pronoun handling and quantificational modifiers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(noun='girl', phrase='The girl'), Entity(noun='table', phrase='the table')]\n",
      "[Triplet(subject=0, relation='approaching', object=1)]\n"
     ]
    }
   ],
   "source": [
    "import sng_parser\n",
    "\n",
    "from deepsight.data.structs import SceneGraph, Entity, Triplet\n",
    "\n",
    "gdict = sng_parser.parse(\"The girl approaching the table\")\n",
    "\n",
    "graph = SceneGraph.new(\n",
    "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
    "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
    ")\n",
    "\n",
    "print(graph.entities())\n",
    "print(graph.triplets(None, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being largely used by many previous works, we noticed that the quality of the parsed scene graphs rapidly degrades as the strcuture of the sentence becomes more distant from *subject* *predicate* *object*. For example, a sentence like \"the girl approaching the table\" is correctly parsed as we can see from the previous python snippet.\n",
    "\n",
    "However, as the sentence becomes more complex, many entities are not found or prepositions/adjectives are classified as entities. Similarly, many relations are missing or the wrong relation is assigned to a pair of entities. For example, if we simply extend the previous sentence with a coordinate conjunction (\"while holding a glass\"), the parser completely ignores the relation `(0, \"holding\", 2)`, making the entity `the glass` not present in the scene graph (as it will be described later, the generated scene graph will be pruned to remove entities not connected to the subject of the description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(noun='girl', phrase='The girl'), Entity(noun='table', phrase='the table'), Entity(noun='glass', phrase='a glass')]\n",
      "[Triplet(subject=0, relation='approaching', object=1)]\n"
     ]
    }
   ],
   "source": [
    "gdict = sng_parser.parse(\"The girl approaching the table while holding a glass\")\n",
    "\n",
    "graph = SceneGraph.new(\n",
    "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
    "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
    ")\n",
    "\n",
    "print(graph.entities())\n",
    "print(graph.triplets(None, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we consider the sentence *\"There is a truck covered in snow farthest from the right\"*, the parser completely ignores the entity `the snow` and the relation `(0, \"covered in\", 2)`. Furthermore, the parser wrongly classified the expression `farthest from the right` as relation + entity, when they should be considered attributes of the entity `the truck`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(noun='truck', phrase='a truck'), Entity(noun='right', phrase='the right')]\n",
      "[Triplet(subject=0, relation='from', object=1)]\n"
     ]
    }
   ],
   "source": [
    "gdict = sng_parser.parse(\"There is a truck covered in snow farthest from the right\")\n",
    "\n",
    "graph = SceneGraph.new(\n",
    "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
    "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
    ")\n",
    "\n",
    "print(graph.entities())\n",
    "print(graph.triplets(None, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the functioning of the parser, we noticed that its poor results are mainly due to the NLP pipeline used for the generation of the dependency graph and to the limited set of rules used to convert the dependency graph into a scene graph. In particular, due to the low quality of many sentences in the dataset (e.g., typos, not perfect syntactic structure), the dependency graph generated by the spaCy `en_core_web_sm` pipeline in many cases assign the wrong universal dependency relation tag between two words, thus leading to the wrong conversion into a scene graph.\n",
    "\n",
    "Thus, since the quality of the scene graph is paramount for the success of the model, we tried to develop a new tool using a more powerful dependency parser and a more refined set of rules. In particular, we used one of the state-of-the-art dependency parsers by ([Attardi et al. 2022](https://github.com/Unipisa/diaparser)), that extends the architecture of the Biaffine Parser by exploiting both embeddings and attentions provided by transformers. Then, based on the dependency graphs generated for some sentences of the dataset and the corresponding ground-truth scene graphs, we developed a new set of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from diaparser.parsers import Parser\n",
    "import rustworkx as rx\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Word:\n",
    "    tag: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DepEntity:\n",
    "    head: list[int]\n",
    "    others: list[int]\n",
    "\n",
    "\n",
    "class ERParser:\n",
    "    def __init__(self) -> None:\n",
    "        self._parser = Parser.load(\"en_ewt-electra\")\n",
    "\n",
    "    def _get_dependency_graph(self, sentence: str) -> rx.PyDiGraph:\n",
    "        dataset = self._parser.predict(sentence, text=\"en\")\n",
    "        tokens = dataset.sentences[0].to_tokens()\n",
    "\n",
    "        graph = rx.PyDiGraph()  # type: ignore\n",
    "\n",
    "        graph.add_node(Word(\"ROOT\", \"-ROOT-\"))\n",
    "        graph.add_nodes_from([Word(token[\"deprel\"], token[\"form\"]) for token in tokens])\n",
    "\n",
    "        for token in tokens:\n",
    "            head_id = int(token[\"head\"])\n",
    "            id = int(token[\"id\"])\n",
    "            graph.add_edge(head_id, id, None)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _get_child_by_tag(self, graph: rx.PyDiGraph, node: int, tag: str) -> list[int]:\n",
    "        children = []\n",
    "\n",
    "        for child_id in graph.neighbors(node):\n",
    "            if tag in graph.get_node_data(child_id).tag:\n",
    "                children.append(child_id)\n",
    "\n",
    "        return children\n",
    "\n",
    "    def _compose_span(self, dep_graph: rx.PyDiGraph, words_ids: list[int]) -> str:\n",
    "        words_ids.sort()\n",
    "        words = [dep_graph.get_node_data(id).text for id in words_ids]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def _get_all_children(self, dep_graph: rx.PyDiGraph, node_id: int) -> list[int]:\n",
    "        children = []\n",
    "        for child_id in dep_graph.neighbors(node_id):\n",
    "            children.append(child_id)\n",
    "            children.extend(self._get_all_children(dep_graph, child_id))\n",
    "        return children\n",
    "\n",
    "    def _compose(\n",
    "        self,\n",
    "        orig: rx.PyDiGraph,\n",
    "        other: rx.PyDiGraph,\n",
    "        parent_id: int,\n",
    "        rel_ids: list[int],\n",
    "    ) -> None:\n",
    "        roots = []\n",
    "        for node_id in other.node_indexes():\n",
    "            if other.in_degree(node_id) == 0:\n",
    "                roots.append(node_id)\n",
    "\n",
    "        if len(roots) == 0:\n",
    "            raise ValueError(\"No root found\")\n",
    "\n",
    "        new_node_ids = orig.compose(other, {})\n",
    "\n",
    "        for root_id in roots:\n",
    "            orig.add_edge(parent_id, new_node_ids[root_id], rel_ids)\n",
    "    \n",
    "    def _get_coordinated_verbs(self, dep_graph: rx.PyDiGraph, verb_id: int) -> list[int]:\n",
    "        verbs = [verb_id]\n",
    "        for child_id in dep_graph.neighbors(verb_id):\n",
    "            tag = dep_graph.get_node_data(child_id).tag\n",
    "            if \"conj\" in tag or \"parataxis\" in tag:\n",
    "                cc_ids = self._get_child_by_tag(dep_graph, child_id, \"cc\")\n",
    "                for cc_id in cc_ids:\n",
    "                    dep_graph.remove_edge(child_id, cc_id)\n",
    "                \n",
    "                punct_ids = self._get_child_by_tag(dep_graph, child_id, \"punct\")\n",
    "                for punct_id in punct_ids:\n",
    "                    dep_graph.remove_edge(child_id, punct_id)\n",
    "                \n",
    "                dep_graph.remove_edge(verb_id, child_id)\n",
    "                verbs.append(child_id)\n",
    "        return verbs\n",
    "\n",
    "    def _parse_noun(self, dep_graph: rx.PyDiGraph, noun_id: int) -> rx.PyDiGraph:\n",
    "        graph = rx.PyDiGraph()\n",
    "\n",
    "        noun_ids = DepEntity([noun_id], [])\n",
    "        noun_node_id = graph.add_node(noun_ids)\n",
    "\n",
    "        for child_id in dep_graph.neighbors(noun_id):\n",
    "            child = dep_graph.get_node_data(child_id)\n",
    "\n",
    "            if dep_graph.out_degree(child_id) == 0:\n",
    "                if \"compound\" in child.tag:\n",
    "                    noun_ids.head.append(child_id)\n",
    "                else:\n",
    "                    noun_ids.others.append(child_id)\n",
    "                continue\n",
    "\n",
    "            if \"det\" in child.tag:\n",
    "                # this determiner should have no children\n",
    "                raise NotImplementedError\n",
    "            elif \"amod\" in child.tag:\n",
    "                obl_ids = self._get_child_by_tag(dep_graph, child_id, \"obl\")\n",
    "                if len(obl_ids) == 0:\n",
    "                    noun_ids.others.append(child_id)\n",
    "                    noun_ids.others.extend(self._get_all_children(dep_graph, child_id))\n",
    "                else:\n",
    "                    for obl_id in obl_ids:\n",
    "                        dep_graph.remove_edge(child_id, obl_id)\n",
    "                        case_ids = self._get_child_by_tag(dep_graph, obl_id, \"case\")\n",
    "                        for case_id in case_ids:\n",
    "                            dep_graph.remove_edge(obl_id, case_id)\n",
    "\n",
    "                    rel_ids = [child_id, *case_ids]\n",
    "                    for obl_id in obl_ids:\n",
    "                        sub_graph = self._parse_noun(dep_graph, obl_id)\n",
    "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
    "            elif \"compound\" in child.tag:\n",
    "                raise NotImplementedError\n",
    "            elif \"nmod\" in child.tag or \"obl\" in child.tag:\n",
    "                case_ids = self._get_child_by_tag(dep_graph, child_id, \"case\")\n",
    "                if len(case_ids) == 0:\n",
    "                    raise ValueError(\"No case found\")\n",
    "                for case_id in case_ids:\n",
    "                    dep_graph.remove_edge(child_id, case_id)\n",
    "                    sub_graph = self._parse_noun(dep_graph, child_id)\n",
    "                    self._compose(graph, sub_graph, noun_node_id, [case_id])\n",
    "            elif \"acl:relcl\" in child.tag:\n",
    "                verb_ids = self._get_coordinated_verbs(dep_graph, child_id)\n",
    "                for verb_id in verb_ids:\n",
    "                    nsubj_ids = self._get_child_by_tag(dep_graph, verb_id, \"nsubj\")\n",
    "                    if len(nsubj_ids) == 0:\n",
    "                        raise ValueError(\"No nsubj found\")\n",
    "                    if len(nsubj_ids) > 1:\n",
    "                        raise ValueError(\"More than one nsubj found\")\n",
    "                    \n",
    "                    dep_graph.remove_edge(verb_id, nsubj_ids[0])\n",
    "                    rel_ids, sub_graph = self._parse_verb(dep_graph, verb_id)\n",
    "                    if sub_graph.num_nodes() == 0:\n",
    "                        noun_ids.others.extend(rel_ids)\n",
    "                    else:\n",
    "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
    "            elif \"acl\" in child.tag or \"root\" in child.tag:\n",
    "                verb_ids = self._get_coordinated_verbs(dep_graph, child_id)\n",
    "                for verb_id in verb_ids:\n",
    "                    rel_ids, sub_graph = self._parse_verb(dep_graph, verb_id)\n",
    "                    if sub_graph.num_nodes() == 0:\n",
    "                        noun_ids.others.extend(rel_ids)\n",
    "                    else:\n",
    "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
    "            elif \"conj\" in child.tag:\n",
    "                cc_ids = self._get_child_by_tag(dep_graph, child_id, \"cc\")\n",
    "                for cc_id in cc_ids:\n",
    "                    dep_graph.remove_edge(child_id, cc_id)\n",
    "\n",
    "                punct_ids = self._get_child_by_tag(dep_graph, child_id, \"punct\")\n",
    "                for punct_id in punct_ids:\n",
    "                    dep_graph.remove_edge(child_id, punct_id)\n",
    "\n",
    "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
    "                graph.compose(sub_graph, {})\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown tag: {child.tag}\")\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _parse_verb(\n",
    "        self, dep_graph: rx.PyDiGraph, verb_id: int\n",
    "    ) -> tuple[list[int], rx.PyDiGraph]:\n",
    "        graph = rx.PyDiGraph()\n",
    "        rel_ids = [verb_id]\n",
    "\n",
    "        for child_id in dep_graph.neighbors(verb_id):\n",
    "            tag = dep_graph.get_node_data(child_id).tag\n",
    "\n",
    "            if dep_graph.out_degree(child_id) == 0:\n",
    "                if \"punct\" not in tag:\n",
    "                    rel_ids.append(child_id)\n",
    "                continue\n",
    "\n",
    "            if \"aux\" in tag:\n",
    "                # this auxiliary verb should have no children\n",
    "                raise NotImplementedError\n",
    "            elif \"nsubj\" in tag:\n",
    "                raise ValueError(\"nsubj should be the root\")\n",
    "            elif \"nmod\" in tag or \"obl\" in tag:\n",
    "                case_ids = self._get_child_by_tag(dep_graph, child_id, \"case\")\n",
    "                if len(case_ids) == 0:\n",
    "                    raise ValueError(\"No case found\")\n",
    "\n",
    "                for case_id in case_ids:\n",
    "                    rel_ids.append(case_id)\n",
    "                    dep_graph.remove_edge(child_id, case_id)\n",
    "\n",
    "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
    "                graph.compose(sub_graph, {})\n",
    "            elif \"obj\" in tag:\n",
    "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
    "                graph.compose(sub_graph, {})\n",
    "            elif \"conj\" in tag:\n",
    "                raise ValueError(\"conj should have been removed\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown tag {tag}\")\n",
    "\n",
    "        return rel_ids, graph\n",
    "\n",
    "    def parse(self, sentence: str) -> SceneGraph:\n",
    "        dep_graph = self._get_dependency_graph(sentence)\n",
    "\n",
    "        root = list(dep_graph.out_edges(0))[0][1]\n",
    "        dep_graph.remove_node(0)\n",
    "\n",
    "        nsubj_ids = self._get_child_by_tag(dep_graph, root, \"nsubj\")\n",
    "        if len(nsubj_ids) == 0:\n",
    "            tmp = self._parse_noun(dep_graph, root)\n",
    "        elif len(nsubj_ids) > 1:\n",
    "            raise ValueError(\"More than one nsubj found\")\n",
    "        else:\n",
    "            dep_graph.remove_edge(root, nsubj_ids[0])\n",
    "            dep_graph.add_edge(nsubj_ids[0], root, None)\n",
    "            tmp = self._parse_noun(dep_graph, nsubj_ids[0])\n",
    "\n",
    "        entities = []\n",
    "        for span_ids in tmp.nodes():\n",
    "            head = self._compose_span(dep_graph, span_ids.head)\n",
    "            span = self._compose_span(dep_graph, span_ids.head + span_ids.others)\n",
    "\n",
    "            entities.append(Entity(head, span))\n",
    "\n",
    "        relations = []\n",
    "        for edge in tmp.edge_list():\n",
    "            subject = edge[0]\n",
    "            object = edge[1]\n",
    "            span_ids = tmp.get_edge_data(subject, object)\n",
    "            predicate = self._compose_span(dep_graph, span_ids)\n",
    "\n",
    "            relations.append(Triplet(subject, predicate, object))\n",
    "\n",
    "        return SceneGraph.new(entities, relations)\n",
    "    \n",
    "\n",
    "parser = ERParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(noun='boy', phrase='The boy'), Entity(noun='friends', phrase='his friends'), Entity(noun='shirt', phrase='a white shirt')]\n",
      "[Triplet(subject=0, relation='having dinner with', object=1), Triplet(subject=0, relation='wearing', object=2)]\n"
     ]
    }
   ],
   "source": [
    "graph = parser.parse(\"The boy wearing a white shirt having dinner with his friends\")\n",
    "\n",
    "print(graph.entities())\n",
    "print(graph.triplets(None, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the promising results obtained on some sentences, we abandoned this approach for two reasons:\n",
    "1. The quality of the resulting scene graph depends too much on the quality of the dependency graph generated by the parser. In particular, even though the new parser correclty parses more complex sentences, it still fails at handling long-range connections between words or relations that can be inferred from common sense. For example, in the sentence *\"There is a truck covered in snow farthest from the right\"*, the parser connects the clause `farthest from the right` to the clause `covered in snow`. While this interpretation of the sentence may be deemed right (the sentence is ambiguous), the preferred interpretation should be that `farthest from the right` is an attribute of the entity `the truck`. However, the parser is not able to infer this relation, thus leading to the wrong scene graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"247ca1a4f0be4ccf91e0cce0a417a935-0\" class=\"displacy\" width=\"1490\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">There</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">truck</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">covered</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">snow</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">farthest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">from</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">right</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\"></tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,162.0 524.0,162.0 524.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-3\" stroke-width=\"2px\" d=\"M302,182.0 302,142.0 527.0,142.0 527.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M527.0,184.0 L531.0,176.0 523.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-4\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M644.0,184.0 L648.0,176.0 640.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-5\" stroke-width=\"2px\" d=\"M782,182.0 782,162.0 884.0,162.0 884.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M782,184.0 L778,176.0 786,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-6\" stroke-width=\"2px\" d=\"M662,182.0 662,142.0 887.0,142.0 887.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M887.0,184.0 L891.0,176.0 883.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-7\" stroke-width=\"2px\" d=\"M662,182.0 662,122.0 1010.0,122.0 1010.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1010.0,184.0 L1014.0,176.0 1006.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-8\" stroke-width=\"2px\" d=\"M1142,182.0 1142,142.0 1367.0,142.0 1367.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1142,184.0 L1138,176.0 1146,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-9\" stroke-width=\"2px\" d=\"M1262,182.0 1262,162.0 1364.0,162.0 1364.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1262,184.0 L1258,176.0 1266,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-10\" stroke-width=\"2px\" d=\"M1022,182.0 1022,122.0 1370.0,122.0 1370.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-247ca1a4f0be4ccf91e0cce0a417a935-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1370.0,184.0 L1374.0,176.0 1366.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sent = parser._parser.predict(\"There is a truck covered in snow farthest from the right\", text=\"en\").sentences[0]\n",
    "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Defining a set of universal rules to handle all possible structures of English sentences (even malformed ones) is extremely hard. In particular, there are many sentences that have very similar dependency graphs that, however, should be transformed in different scene graphs, thus requiring not only to handle the structure of the sentence, but also its semantics. For example, the expressions *\"the part of the table\"*, *\"the first horse from the left\"* and *\"the woman in green clothes\"*, have very similar dependency graphs, but in the first two cases the expressions should be considered a single entity, while in the last case they should be considered two different entities related by the relation `wearing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b034390e866d4f06859bf41ebfa00905-0\" class=\"displacy\" width=\"770\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">part</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">table</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b034390e866d4f06859bf41ebfa00905-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b034390e866d4f06859bf41ebfa00905-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b034390e866d4f06859bf41ebfa00905-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b034390e866d4f06859bf41ebfa00905-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b034390e866d4f06859bf41ebfa00905-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,142.0 647.0,142.0 647.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b034390e866d4f06859bf41ebfa00905-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b034390e866d4f06859bf41ebfa00905-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b034390e866d4f06859bf41ebfa00905-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b034390e866d4f06859bf41ebfa00905-0-4\" stroke-width=\"2px\" d=\"M302,182.0 302,122.0 650.0,122.0 650.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b034390e866d4f06859bf41ebfa00905-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M650.0,184.0 L654.0,176.0 646.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = parser._parser.predict(\"the part of the table\", text=\"en\").sentences[0]\n",
    "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2781f33e26ad448aba7aee28dd62fad4-0\" class=\"displacy\" width=\"890\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">first</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">horse</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">from</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">left</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\"></tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,142.0 407.0,142.0 407.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-1\" stroke-width=\"2px\" d=\"M302,182.0 302,162.0 404.0,162.0 404.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M302,184.0 L298,176.0 306,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-2\" stroke-width=\"2px\" d=\"M62,182.0 62,122.0 410.0,122.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M410.0,184.0 L414.0,176.0 406.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,142.0 767.0,142.0 767.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-4\" stroke-width=\"2px\" d=\"M662,182.0 662,162.0 764.0,162.0 764.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,184.0 L658,176.0 666,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2781f33e26ad448aba7aee28dd62fad4-0-5\" stroke-width=\"2px\" d=\"M422,182.0 422,122.0 770.0,122.0 770.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2781f33e26ad448aba7aee28dd62fad4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770.0,184.0 L774.0,176.0 766.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = parser._parser.predict(\"the first horse from the left\", text=\"en\").sentences[0]\n",
    "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b281d38220594250a789262d378009cb-0\" class=\"displacy\" width=\"770\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">woman</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">green</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">clothes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b281d38220594250a789262d378009cb-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b281d38220594250a789262d378009cb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b281d38220594250a789262d378009cb-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b281d38220594250a789262d378009cb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b281d38220594250a789262d378009cb-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,142.0 647.0,142.0 647.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b281d38220594250a789262d378009cb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b281d38220594250a789262d378009cb-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b281d38220594250a789262d378009cb-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b281d38220594250a789262d378009cb-0-4\" stroke-width=\"2px\" d=\"M302,182.0 302,122.0 650.0,122.0 650.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b281d38220594250a789262d378009cb-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M650.0,184.0 L654.0,176.0 646.0,176.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = parser._parser.predict(\"the woman in green clothes\", text=\"en\").sentences[0]\n",
    "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Language Models\n",
    "\n",
    "Large Language Models pretrained on large text corpora have recently shown to be able to reach state-of-the-art results on many NLP tasks for which they were not explicitly trained, like question answering, summarization and text generation. Furthermore, since these models seem to show emergent reasoning capabilities ([Liu et al. 2023](https://arxiv.org/abs/2304.03439)), we considered their use for this task that, as previously said, requires common sense and the ability to understand the scene context described by the sentence.\n",
    "\n",
    "In our first attempts, we tried to use open LLM available on the HuggingFace site, like [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), [Alpaca-13B](https://crfm.stanford.edu/2023/03/13/alpaca.html), Stable-Vicuna-13B, given the possibility to use their quantized version and thus running them on single gpus with only 8/16 GB of memory. For all our experiments we adopted a few-shot approach, i.e., in the prompt we described the task and then we provided a set of examples of the task. When generating the prompt, the main tradeoff was between the number of examples (and thus the inference time) and the quality of the generated scene graphs. At the end, we decided to use 5 examples that allowed us to show the model a variety of sentences with different structures and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\"\\\n",
    "The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. Falcon is extremely good at visualizing and understanding a scene from a short description of it, and can answer questions about the scene. User is a human who is curious about the world, and wants to know more about the entities present in the scene, even if not explicitly stated in the description, and which relations occur among them.\n",
    "In the following interactions, User will make requests in natural language, while Falcon will answer to each of these requests with a well formed JSON. The conversation begins.\n",
    "User: Given the following sentence: \"{sentence1}\", what are the entities present in the scene? What are their relations? For each entity, please provide also a single word that best summarizes it and make sure that the subject of the sentence is the first entity.\n",
    "Falcon: {example1}\n",
    "User: Do the same for the following sentence: \"{sentence2}\".\n",
    "Falcon: {example2}\n",
    "User: Try with this one: \"{sentence3}\".\n",
    "Falcon: {example3}\n",
    "User: Please, do the same also for this sentence: \"{sentence4}\".\n",
    "Falcon: {example4}\n",
    "User: Let's try with this one: \"{sentence5}\".\n",
    "Falcon: {example5}\n",
    "User: Finally, try with this one: \"{sentence6}\".\n",
    "Falcon:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "class LLMParser:\n",
    "    def __init__(self) -> None:\n",
    "        name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        self._prompt = prompt\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "        self._pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            batch_size=1, # we found that increasing the batch slows down the generation\n",
    "        )\n",
    "\n",
    "        self._pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    def _create_prompt(self, sentence: str) -> str:\n",
    "        sentence1 = \"the girl looking at the table full of drinks\"\n",
    "        example1 = {\n",
    "            \"entities\": [\n",
    "                (\"the girl\", \"girl\"),\n",
    "                (\"the table\", \"table\"),\n",
    "                (\"drinks\", \"drinks\"),\n",
    "            ],\n",
    "            \"relations\": [\n",
    "                (0, \"looking at\", 1),\n",
    "                (1, \"full of\", 2),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        sentence2 = (\n",
    "            \"the man wearing a long sleeved white shirt and a pair of blue jeans \"\n",
    "            \"catching a freesbie\"\n",
    "        )\n",
    "        example2 = {\n",
    "            \"entities\": [\n",
    "                (\"the man\", \"man\"),\n",
    "                (\"a long sleeved white shirt\", \"shirt\"),\n",
    "                (\"a pair of blue jeans\", \"jeans\"),\n",
    "                (\"a freesbie\", \"freesbie\"),\n",
    "            ],\n",
    "            \"relations\": [\n",
    "                (0, \"wearing\", 1),\n",
    "                (0, \"wearing\", 2),\n",
    "                (0, \"catching\", 3),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        sentence3 = \"Skateboarder in green\"\n",
    "        example3 = {\n",
    "            \"entities\": [\n",
    "                (\"Skateboarder\", \"Skateboarder\"),\n",
    "                (\"green clothes\", \"clothes\"),\n",
    "            ],\n",
    "            \"relations\": [\n",
    "                (0, \"in\", 1),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        sentence4 = \"glass far right\"\n",
    "        example4 = {\n",
    "            \"entities\": [(\"glass far right\", \"glass\")],\n",
    "            \"relations\": [],\n",
    "        }\n",
    "\n",
    "        sentence5 = \"2nd to theleft brown horse drinking\"\n",
    "        example5 = {\n",
    "            \"entities\": [\n",
    "                (\"brown horse drinking\", \"horse\"),\n",
    "                (\"leftmost brown horse\", \"horse\"),\n",
    "            ],\n",
    "            \"relations\": [\n",
    "                (0, \"at the right of\", 1),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        return self._prompt.format(\n",
    "            sentence1=sentence1,\n",
    "            example1=json.dumps(example1),\n",
    "            sentence2=sentence2,\n",
    "            example2=json.dumps(example2),\n",
    "            sentence3=sentence3,\n",
    "            example3=json.dumps(example3),\n",
    "            sentence4=sentence4,\n",
    "            example4=json.dumps(example4),\n",
    "            sentence5=sentence5,\n",
    "            example5=json.dumps(example5),\n",
    "            sentence6=sentence,\n",
    "        )\n",
    "\n",
    "    def parse(self, sentences: Iterable[str]) -> Iterable[SceneGraph]:\n",
    "        \"\"\"\"Extracts scene graphs from a list of sentences.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences : Iterable[str]\n",
    "            A list of sentences to parse.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Iterable[SceneGraph]\n",
    "            A list of scene graphs, one for each sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        prompts = (self._create_prompt(s) for s in sentences)\n",
    "\n",
    "        generator = self._pipe(\n",
    "            prompts,\n",
    "            max_length=1000,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,\n",
    "            # top_k=10,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "        for output in generator:\n",
    "            generated = output[0][\"generated_text\"]\n",
    "            generated = generated.split(\"\\n\")[0]\n",
    "            gen_json = json.loads(generated)\n",
    "\n",
    "            entities = []\n",
    "            for span, head in gen_json[\"entities\"]:\n",
    "                entities.append(Entity(head, span))\n",
    "\n",
    "            triplets = []\n",
    "            for s, pred, o in gen_json[\"relations\"]:\n",
    "                triplets.append(Triplet(s, pred, o))\n",
    "\n",
    "            yield SceneGraph.new(entities=entities, triplets=triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the recent claims on the capabilities of open LLM, we found that the quality of the generated scene graphs was not good enough for our purposes, if not for some simple sentences. Furthermore, the inference time was too high to be able to preprocess the whole dataset in reasonable times with our limited computational resources. For example, on a single A100 GPU, the inference time for a single sentence was around 15 seconds, thus requiring more than 16 days to preprocess the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these reasons, we decided to use ChatGPT (`gpt-3.5-turbo`) through the APIs made available by OpenAI. Indeed, by making ChatGPT parse mutliple sentences (`10`) for each request, we were able to reduce the time required to preprocess the whole dataset to around 10 hours. The number of sentences per request was chosen sufficiently high to reduce the inference time (and the cost of the API calls) but not too high to avoid the model to start hallucinating.\n",
    "\n",
    "Even in this case, when generating the prompt, we had to tradeoff between the number of examples and the quality of the generated scene graphs. In particular, by increasing the number of examples, we were able to show the model a wider variety of sentences, thus improving the quality of the scene graphs. However, this also increased the inference time\n",
    "and the cost of the API calls. At the end, we decided to use 7 examples that showed many typical constructions of the sentences in the dataset (for a short description of the reasons why we chose these examples, see the following code snippet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import enum\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import openai\n",
    "\n",
    "from deepsight.data.structs import SceneGraph\n",
    "\n",
    "\n",
    "class GPTModel(enum.Enum):\n",
    "    GPT3_5 = \"gpt-3.5-turbo\"\n",
    "    GPT4 = \"gpt-4\"\n",
    "\n",
    "    def openai_model(self) -> str:\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class SceneGraphParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str | None = None,\n",
    "        model: GPTModel = GPTModel.GPT3_5,\n",
    "        temperature: float = 0.2,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the parser with the given parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str, optional\n",
    "            OpenAI API key. If not provided, the token will be read from the\n",
    "            environment variable OPENAI_API_KEY.\n",
    "        model : GPTModel, optional\n",
    "            The GPT model to use. Defaults to GPTModel.GPT3_5.\n",
    "        temperature : float, optional\n",
    "            The temperature to use when sampling from the model. Should be between\n",
    "            0 and 2, where higher values will make the output more random, while\n",
    "            lower values will make the output more focused and deterministic.\n",
    "            Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if api_key is None:\n",
    "                raise ValueError(\n",
    "                    \"No OpenAI API key provided. Please provide a key or set the \"\n",
    "                    \"environment variable OPENAI_API_KEY.\"\n",
    "                )\n",
    "\n",
    "        openai.api_key = api_key\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def _build_requests(self, captions: list[str]) -> str:\n",
    "        output = \"\"\n",
    "        for caption in captions:\n",
    "            output += f\"<caption>{caption}</caption>\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _build_examples(self, graphs: list[SceneGraph]) -> str:\n",
    "        output = \"\"\n",
    "        for graph in graphs:\n",
    "            output += f\"<json>{graph.to_dict()}</json>\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _match_entity(self, entity: str, entities: list[dict[str, Any]]) -> int | None:\n",
    "        \"\"\"Matches the given entity to an entity in the list of entities.\"\"\"\n",
    "\n",
    "        for idx, ent in enumerate(entities):\n",
    "            if entity in ent[\"phrase\"]:\n",
    "                return idx\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _get_entity_index(\n",
    "        self, entity: str | int | None, entities: list[dict[str, Any]]\n",
    "    ) -> int | None:\n",
    "        if entity is None:\n",
    "            return None\n",
    "\n",
    "        entity_idx: int\n",
    "        if isinstance(entity, int):\n",
    "            entity_idx = entity\n",
    "        elif entity.isdigit():\n",
    "            entity_idx = int(entity)\n",
    "        else:\n",
    "            idx = self._match_entity(entity, entities)\n",
    "            if idx is None:\n",
    "                entities.append({\"noun\": entity, \"phrase\": entity})\n",
    "                entity_idx = len(entities) - 1\n",
    "            else:\n",
    "                entity_idx = idx\n",
    "\n",
    "        if entity_idx >= len(entities):\n",
    "            return None\n",
    "\n",
    "        return entity_idx\n",
    "\n",
    "    def _postprocess(self, output: dict[str, Any]) -> SceneGraph | None:\n",
    "        entities = output.get(\"entities\", [])\n",
    "        if len(entities) == 0:\n",
    "            return None\n",
    "\n",
    "        triplets = output.get(\"triplets\", [])\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            subj = self._get_entity_index(triplet.get(\"subject\"), entities)\n",
    "            obj = self._get_entity_index(triplet.get(\"object\"), entities)\n",
    "\n",
    "            match (subj, obj):\n",
    "                case (None, None):\n",
    "                    continue\n",
    "                case (None, obj):\n",
    "                    entities[obj][\"phrase\"] = (\n",
    "                        entities[obj][\"phrase\"] + \" \" + triplet[\"relation\"]\n",
    "                    )\n",
    "                case (subj, None):\n",
    "                    entities[subj][\"phrase\"] = (\n",
    "                        entities[subj][\"phrase\"] + \" \" + triplet[\"relation\"]\n",
    "                    )\n",
    "                case (subj, obj):\n",
    "                    new_triplets.append(\n",
    "                        {\n",
    "                            \"subject\": subj,\n",
    "                            \"object\": obj,\n",
    "                            \"relation\": triplet[\"relation\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return SceneGraph.from_dict({\"entities\": entities, \"triplets\": new_triplets})\n",
    "\n",
    "    async def parse(\n",
    "        self, examples: list[tuple[str, SceneGraph]], captions: list[str]\n",
    "    ) -> list[tuple[str, SceneGraph | None]]:\n",
    "        \"\"\"Parses the given captions into scene graphs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : list[tuple[str, SceneGraph]]\n",
    "            A list of examples to use for the prompt. Each example is a tuple\n",
    "            consisting of a caption and the corresponding scene graph.\n",
    "        captions : list[str]\n",
    "            A list of captions to parse into scene graphs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[tuple[str, SceneGraph | None]]\n",
    "            A list of tuples consisting of the original caption and the parsed\n",
    "            scene graph. If the parsing fails due to formatting issues, the scene\n",
    "            graph will be `None`.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If the parsing fails.\n",
    "        openai.error.OpenAIError\n",
    "            The error returned by the OpenAI API.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            res = await openai.ChatCompletion.acreate(\n",
    "                model=self.model.openai_model(),\n",
    "                temperature=self.temperature,\n",
    "                n=1,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    { # we add the example captions as previous requests from the user\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": self._build_requests([cap for cap, _ in examples]),\n",
    "                    },\n",
    "                    { # we add the corresponding scene graphs as previous responses from the assistant\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": self._build_examples(\n",
    "                            [graph for _, graph in examples]\n",
    "                        ),\n",
    "                    },\n",
    "                    { # we add the captions to parse as the next request from the user\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": self._build_requests(captions),\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "        except openai.error.OpenAIError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Input: {captions}\") from e\n",
    "\n",
    "        response: str = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "        outputs = response.split(\"\\n\")\n",
    "\n",
    "        results: list[tuple[str, SceneGraph | None]] = []\n",
    "        for caption, output in zip(captions, outputs):\n",
    "            start = output.find(\"{\")\n",
    "            end = output.rfind(\"}\")\n",
    "            output = output[start : end + 1]\n",
    "\n",
    "            try:\n",
    "                output_dict = ast.literal_eval(output)\n",
    "                graph = self._postprocess(output_dict)\n",
    "                if graph is not None:\n",
    "                    results.append((caption, graph))\n",
    "                else:\n",
    "                    results.append((caption, None))\n",
    "            except Exception as e:\n",
    "                results.append((caption, None))\n",
    "                print(f\"Input: {caption} | Output: {output}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# this is the firt part of the prompt\n",
    "# after this, the examples are added\n",
    "system = \"\"\"\\\n",
    "You will be provided with a set of captions each describing a region in an image. \\\n",
    "For each region, first identify the entities, like people, objects or places, present in the region Specify both a single noun and a phrase that describes the entity. \\\n",
    "Then, identify the triplets of subject, relation and object that describe the relationships between the entities. \\\n",
    "\"\"\"  # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is useful to show the model that the attributes of an entity can be non adjacent to the entity itself.\n",
    "# Furthermore, it shows that the location of an entity in the scene should be considered an attribute of the entity.\n",
    "example1 = (\n",
    "    \"There is a truck covered in snow farthest from the right\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"truck\", \"a truck farthest from the right\"),\n",
    "            Entity(\"snow\", \"snow\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"covered in\", 1),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This example shows that the attributes of an entity do not need to be adjectives or adverbial phrases,\n",
    "# but can also relative clauses.\n",
    "example2 = (\n",
    "    \"A placemat is empty behind a placemat that is full\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"placemat\", \"an empty placemat\"),\n",
    "            Entity(\"placemat\", \"a full placemat\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"behind\", 1),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This example condenses in one sentence the information provided by the two previous examples.\n",
    "example3 = (\n",
    "    \"the chair not being used in the background, perpendicular to the viewer\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"chair\", \"the chair not being used in the background\"),\n",
    "            Entity(\"viewer\", \"the viewer\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"perpendicular to\", 1),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This example was added because in the dataset we found many descriptions of this form,\n",
    "# \"the book with the title X\" and we found that the model was not able to parse them.\n",
    "example4 = (\n",
    "    \"A double decker bus with the wording The Ghost Bus Tours.com on the side.\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"bus\", \"a double decker bus\"),\n",
    "            Entity(\"wording\", \"the wording The Ghost Bus Tours.com\"),\n",
    "            Entity(\"side\", \"the side\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"with\", 1),\n",
    "            Triplet(1, \"on\", 2),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This example shows a quite complex sentence, with multiple entities and relations.\n",
    "# In particular, it shows the model that a pronoun should not be parsed as a new entity, but as a reference to an existing entity,\n",
    "# thus all relations involving the pronoun should be between the subject/object of the relation and the entity the pronoun refers to.\n",
    "# Furthemore, it shows that thr subject of the caption does not need to be the subject of all relations, but can also be the object of some relations.\n",
    "example5 = (\n",
    "    \"a man stands majestically on his skis on a snow covered area with 2 other people \"\n",
    "    + \"behind him in the distance\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"man\", \"a man\"),\n",
    "            Entity(\"ski\", \"his skis\"),\n",
    "            Entity(\"snow area\", \"a snow covered area\"),\n",
    "            Entity(\"people\", \"2 other people\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"stands on\", 1),\n",
    "            Triplet(0, \"on\", 2),\n",
    "            Triplet(3, \"behind\", 0),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This example shows a quite simple and linear sentence, similar to many of the sentences in the dataset.\n",
    "# In particular, it shows the model that the expression \"<subject> in <clothing>\" (largely present in the dataset) \n",
    "# should be parsed as two different entities, one for the subject and one for the clothing (in a relation) and not as a single entity.\n",
    "example6 = (\n",
    "    \"Woman in white shirt looking down at laptop computer and \" + \"holding a glass\",\n",
    "    SceneGraph.new(\n",
    "        entities=[\n",
    "            Entity(\"woman\", \"woman\"),\n",
    "            Entity(\"shirt\", \"white shirt\"),\n",
    "            Entity(\"computer\", \"laptop computer\"),\n",
    "            Entity(\"glass\", \"a glass\"),\n",
    "        ],\n",
    "        triplets=[\n",
    "            Triplet(0, \"in\", 1),\n",
    "            Triplet(0, \"looking down at\", 2),\n",
    "            Triplet(0, \"holding\", 3),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This simple example was added because we found that, based on the previous examples,\n",
    "# the model still did not correctly parsed the location of an entity as an attribute of the entity.\n",
    "example7 = (\n",
    "    \"Woman on the right\",\n",
    "    SceneGraph.new(entities=[Entity(\"woman\", \"woman on the right\")], triplets=[]),\n",
    ")\n",
    "\n",
    "examples = [example1, example2, example3, example4, example5, example6, example7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "gpt_parser = SceneGraphParser(\"api key\") # for testing, we can provide a temporary api key\n",
    "\n",
    "sentence = \"The girl approaching the table while holding a glass\"\n",
    "graph = asyncio.run(gpt_parser.parse(examples, [sentence]))[0][1]\n",
    "if graph is not None: # if the parsing was successful\n",
    "    print(graph.entities())\n",
    "    print(graph.triplets(None, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations on the quality of the generated scene graphs:\n",
    "1. Among the methods previously illustrated, the graph generated with ChatGPT are by far the best ones, but they are still far from perfect. In particular, we noticed that ChatGPT is inconsistent in the generated scene graphs, i.e., clauses with extremely similar structures (and thus corresponding scene graphs) are associated to different scene graphs. For example, spatial locations are not always treated as attributes of the entity they refer to, but they are sometimes parsed as spatial relations, despite the mutliple example in the prompt. Furthermore, we noticed that sometimes the text associated to a relation is also associated to one of the entities involved.\n",
    "2. The textual descriptions of the parsed entities and relations are strongly based on the words present in the input sentence. For example, in a sentence like *\"the woman in a green shirt\"*, the relation is described simply using the preposition `in` like it is in the sentence. While this is not wrong, a better description of the relation would be `wearing`. Similarly, in the sentence *\"the zebra walking with its young one*\", the detected entities are `the zebra` and `its young one` which is not wrong, but a better description would be `the zebra` and `the young zebra`. When generating the prompt, we tried to teach the model to not necessarily use the words present in the sentence, but the model started to hallucinate thus compromising the quality of the generated scene graphs. <br>\n",
    "We noticed similar results when we tried to make the model generate different relations based on whether the subject of the relation in the scene graph is also the subject of the relation in the sentence. For example, given a sentence like *\"the girl looking at the table\"*, the generated scene graph would be a graph with two nodes (`the girl`, `the table`) and an undirected edge between the two representing the relation `looking at`. We tried to make the model create a directed scene graph with the relation `looking at` from `the girl` to `the table` and the relation `being looked at` from `the table` to `the girl`, but the model started to hallucinate. <br>\n",
    "Notice that GPT4 is instead able to generate such more complex scene graphs, showing that it has a better \"understading\" of the content of the sentence. Unfortunately, the GPT4 API were not openly available at the time of the project and their cost is 20x higher than the cost of the ChatGPT API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture modules\n",
    "\n",
    "The developed framework decomposes the model architecture (here called `Pipeline`) into four main modules:\n",
    "- `PreProcessor`: module used to transform the input data before feeding it to the model. Notice that such module at training time takes in input also the target data since it may be necessary for implementing some training strategies like denoising bounding box coordinates ([Li et al. 2022](https://arxiv.org/abs/2203.01305), [Zhang et al. 2022](https://arxiv.org/abs/2203.03605)).\n",
    "- `Model`: this is the core of the pipeline and it is responsible for all the main computations.\n",
    "- `PostProcessor`: module used to transform the output of the model into the same format of the target data.\n",
    "- `Criterion`: module used to compute the loss between the output of the model and the target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcessor\n",
    "\n",
    "In the preprocessor, we perform two main preprocessing steps. First, we resize each image such that its shortest size is 800px keeping its original aspect ratio. If by doing so, the longest size is longer than 1333px, then we resize the image such that its longest size is 1333px (still keeping the aspect ratio). This is the same strategy used by Grounding DINO. The we standardize the image using the channels mean and standard deviation used by CLIP.\n",
    "\n",
    "The preprocessor is also the step where the scene graph is generated. Since generating the scene graph using ChatGPT has a high latency (more than 10 seconds), it is not feasible to perform this step in real time. Thus, we preprocessed the whole dataset before training the model. The generated scene graphs are then stored in a json file that is loaded by the preprocessor at initialization.\n",
    "\n",
    "Since the candidate bounding boxes are generated from the embeddings of the subject nodes, in case the generated scene graph has more than one connected component, we remove from the scene graph all components except the one containing the subject entity. In fact, since during the `Graph Attention` step, each node can pass directly (or indirectly thorugh its neighbours) messages only with the nodes in the same connected component, nodes in different connected components would not be able to exchange messages and thus influence the embeddings of the subject nodes. Keeping them would thus be useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "from deepsight.data.structs import Batch, RECInput, RECOutput, SceneGraph\n",
    "from deepsight.data.transformations import Compose, Resize, Standardize\n",
    "from deepsight.modeling.pipeline import PreProcessor as _PreProcessor\n",
    "from deepsight.utils.torch import Batched3DTensors\n",
    "\n",
    "from projects.sgg.modeling import PreprocessorConfig\n",
    "from projects.sgg.modeling._structs import ModelInput\n",
    "\n",
    "\n",
    "class PreProcessor(_PreProcessor[RECInput, RECOutput, ModelInput]):\n",
    "    def __init__(self, config: PreprocessorConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self._preparsed: dict[str, dict[str, Any]] = {}\n",
    "        if config.file is not None:\n",
    "            with config.file.open(\"r\") as f:\n",
    "                self._preparsed = json.load(f)\n",
    "\n",
    "        # self._parser = gpt.SceneGraphParser(config.token)\n",
    "\n",
    "        self._transform = Compose(\n",
    "            [\n",
    "                Resize(config.side, max_size=config.max_side, p=1.0),\n",
    "                Standardize(config.mean, config.std, p=1.0),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: Batch[RECInput],\n",
    "        targets: Batch[RECOutput] | None,\n",
    "    ) -> ModelInput:\n",
    "        graphs = []\n",
    "        for inp in inputs:\n",
    "            if inp.description in self._preparsed:\n",
    "                scene_graph = SceneGraph.from_dict(self._preparsed[inp.description])\n",
    "                # remove all nodes not connected to the root node\n",
    "                # since they will never pass messages to the root node\n",
    "                # (directly or through other nodes)\n",
    "                scene_graph = scene_graph.node_connected_component(0)\n",
    "                graphs.append(scene_graph)\n",
    "            else:\n",
    "                # we currently do not support real-time scene graph generation\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return ModelInput(\n",
    "            images=[i.image for i in inputs],\n",
    "            features=Batched3DTensors.from_list(\n",
    "                [self._transform(inp.image)[0].to_tensor().data for inp in inputs]\n",
    "            ),\n",
    "            captions=[i.description for i in inputs],\n",
    "            graphs=graphs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "As previously said, the model consists of four main components:\n",
    "1. Vision Encoder\n",
    "2. Text Encoder\n",
    "3. Object Detector\n",
    "4. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vision Encoder\n",
    "\n",
    "The vision encoder used is a modified version of the ViT encoder used by CLIP. In particular, the original CLIP vision encoder returns a simgle embedding for each image. In our case, we need it to return a feature map for each image to allow each query to attend to different parts of the image. Thus, we modified the cLIP implementation by removing the attention-based pooling and taking the patch embeddings of the last layer as the feature map. Similarly to OwlViT ([Minderer et al. 2022](https://arxiv.org/abs/2205.06230)), we further multiply each patch embedding with the class token and we apply a layer norm. The authors of OwlViT state that this last operation improves the performance of the model, but no explanation if given. Probably, this allow each patch to encode information of the whole image and not only of a specific region. Since the feature dimension of each patch is 768, we also apply a final linear projection to reduce the dimension to 256 (the same dimension used by DETR-like models). To avoid losing all the information of the discarded linear projection, we initialize the weights by applying PCA to the original weights (even though no significant improvement was observed with respect to random initialization).\n",
    "\n",
    "Another difference is that the original CLIP encoder resizes and center-crops each image to a 224x224 format. By resizing the image to such a small dimension, small details in the image may be lost, that may be important for the object detection task. Firthermore, by center cropping the image, some parts of the image thatmay contain some of the entities referred in the sentence may be cropped out. Thus, as previously said, we resize each image to a larger size and we do not apply cropping. This is similar to what is done by OwlViT. However, while OwlViT resizes each image to a fixed size, we keep the original aspect ratio and we use padding (with attention masking) to handle images of different sizes in the same batch. Similarly to OwlViT, we do not discard the learned positional embeddings of CLIP, but we simply interpolate them to the new image size.\n",
    "\n",
    "Note: We used the CLIP vision encoder based on ViT instead of ResNet-50 because we verified that when giving in input to the encoder images with a larger size than what the model was trained on, the final embeddings were more similar to the embeddings obtained by using the 224x224 image. Thus it seems that the ViT encoder is more robust to changes in the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import Tensor, nn\n",
    "from transformers.models.clip.modeling_clip import (\n",
    "    CLIPVisionEmbeddings,\n",
    "    CLIPVisionModelWithProjection,\n",
    ")\n",
    "\n",
    "from deepsight.utils.torch import Batched2DTensors, Batched3DTensors\n",
    "\n",
    "from deepsight.modeling.layers.clip._misc import Models\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"A modified version of the CLIP [1]_ vision encoder.\n",
    "\n",
    "    There are two main differences:\n",
    "    - While the CLIP vision encoder returns a single vector representation for each\n",
    "    image by pooling the patch embeddings, this encoder returns a 2D feature map for\n",
    "    each image. Similarly to OwlViT [2]_, the 2D feature map is obtained by multiplying\n",
    "    the patches with the class token and applying a layer norm. Each patch is then\n",
    "    projected to the output dimension using a linear layer.\n",
    "    - While the CLIP vision encoder requires all images to be rescaled to the same\n",
    "    size (224x224 or 336x336), this encoder does not require images to be rescaled to\n",
    "    the same fixed size. Instead, the positional embeddings are interpolated to the size\n",
    "    of each image. This should improve the performance of the encoder on large images\n",
    "    with fine-grained details.\n",
    "\n",
    "    .. note::\n",
    "        The CLIP vision encoder has an output dimension of 512 that is double what is\n",
    "        used by most object detection models. Thus, if the specified `output_dim` is\n",
    "        not 512, the projection layer is replaced with a linear layer that has the\n",
    "        specified output dimension. The weights of the linear layer are initialized by\n",
    "        applying PCA to the weights of the original projection layer.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,\n",
    "        S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.\n",
    "        Learning transferable visual models from natural language supervision.\n",
    "        In International conference on machine learning (pp. 8748-8763). PMLR.\n",
    "    .. [2] Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D.,\n",
    "        Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z. and Wang, X.,\n",
    "        2022, October. Simple open-vocabulary object detection. In European Conference\n",
    "        on Computer Vision (pp. 728-755). Cham: Springer Nature Switzerland.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Models, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        clip = CLIPVisionModelWithProjection.from_pretrained(model.weights())\n",
    "\n",
    "        vision = clip.vision_model\n",
    "        self.embeddings = VisionEmbeddings(vision.embeddings)\n",
    "        self.pre_layernorm = vision.pre_layrnorm\n",
    "        self.encoder = vision.encoder\n",
    "        self.post_layernorm = vision.post_layernorm\n",
    "\n",
    "        self.last_layernorm = nn.LayerNorm(clip.config.hidden_size)\n",
    "\n",
    "        projection = clip.visual_projection\n",
    "        if projection.out_features != output_dim:\n",
    "            weights = projection.weight.transpose(0, 1).detach().numpy()\n",
    "            weights = PCA(output_dim).fit_transform(weights)\n",
    "            self.projection = nn.Linear(\n",
    "                in_features=projection.in_features,\n",
    "                out_features=output_dim,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.projection.weight = nn.Parameter(\n",
    "                    torch.from_numpy(weights).transpose(0, 1)\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            self.projection = projection\n",
    "\n",
    "    def _create_attention_mask(self, x: Batched2DTensors) -> Float[Tensor, \"B 1 L L\"]:\n",
    "        \"\"\"Creates an attention mask to mask out the padding tokens.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Batched2DTensors\n",
    "            The input flattened image tensors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Float[Tensor, \"B 1 L L\"]\n",
    "            The attention mask.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = x.mask[:, None, None, :].expand(-1, 1, x.shape[1], -1)\n",
    "\n",
    "        dtype = x.tensor.dtype\n",
    "        attn_mask = torch.zeros_like(mask, dtype=dtype)\n",
    "        attn_mask.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, images: Batched3DTensors) -> Batched3DTensors:\n",
    "        x, new_sizes = self.embeddings(images)  # (B, 1+HW, C)\n",
    "\n",
    "        attn_mask = self._create_attention_mask(x)\n",
    "\n",
    "        hidden: Tensor = self.pre_layernorm(x.tensor)\n",
    "        tmp = self.encoder(\n",
    "            inputs_embeds=hidden,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            attention_mask=attn_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        hidden = tmp.last_hidden_state  # (B, 1+HW, C)\n",
    "        hidden = self.post_layernorm(hidden)\n",
    "\n",
    "        class_token = hidden[:, :1]  # (B, 1, C)\n",
    "        image_embeds = hidden[:, 1:]  # (B, HW, C)\n",
    "\n",
    "        out: Tensor = class_token * image_embeds\n",
    "        out = self.last_layernorm(out)\n",
    "\n",
    "        out = self.projection(out)  # (B, HW, D)\n",
    "\n",
    "        H = max(size[0] for size in new_sizes)\n",
    "        W = max(size[1] for size in new_sizes)\n",
    "\n",
    "        out = out.view(out.shape[0], H, W, -1).permute(0, 3, 1, 2)  # (B, D, H, W)\n",
    "\n",
    "        return Batched3DTensors(out, sizes=new_sizes)\n",
    "\n",
    "    def __call__(self, images: Batched3DTensors) -> Batched3DTensors:\n",
    "        return super().__call__(images)  # type: ignore\n",
    "\n",
    "\n",
    "class VisionEmbeddings(nn.Module):\n",
    "    \"\"\"A wrapper around the CLIP vision embeddings.\n",
    "\n",
    "    This wrapper allows the CLIP vision encoder to work with batches of images of\n",
    "    different sizes. To avoid discarding the learned positional embeddings, the\n",
    "    positional embeddings are interpolated to the size of each image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings: CLIPVisionEmbeddings) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = embeddings.patch_embedding\n",
    "        self.class_embedding = embeddings.class_embedding\n",
    "\n",
    "        h, w = (int(embeddings.num_patches**0.5),) * 2\n",
    "        patch_pos_embedding = embeddings.position_embedding.weight.data[1:]\n",
    "        patch_pos_embedding = patch_pos_embedding.reshape(h, w, -1).permute(2, 0, 1)\n",
    "        class_pos_embedding = embeddings.position_embedding.weight.data[0]\n",
    "\n",
    "        self.patch_pos_embedding = nn.Parameter(patch_pos_embedding)\n",
    "        self.class_pos_embedding = nn.Parameter(class_pos_embedding)\n",
    "\n",
    "    def _compute_new_size(self, old_size: tuple[int, int]) -> tuple[int, int]:\n",
    "        \"\"\"Computes the new size of the image after patch embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_size : tuple[int, int]\n",
    "            The size of the image before patch embedding.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[int, int]\n",
    "            The size of the image after patch embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        kh, kw = self.patch_embedding.kernel_size\n",
    "        sh, sw = self.patch_embedding.stride\n",
    "        ph, pw = self.patch_embedding.padding\n",
    "\n",
    "        H, W = old_size\n",
    "        h = (H + 2 * ph - kh) // sh + 1\n",
    "        w = (W + 2 * pw - kw) // sw + 1\n",
    "\n",
    "        return h, w\n",
    "\n",
    "    def forward(\n",
    "        self, images: Batched3DTensors\n",
    "    ) -> tuple[Batched2DTensors, list[tuple[int, int]]]:\n",
    "        B = len(images)\n",
    "        x: Tensor = self.patch_embedding(images.tensor)\n",
    "\n",
    "        new_sizes = []\n",
    "        patch_pos_emb = torch.zeros_like(x)  # (B, C, H, W)\n",
    "\n",
    "        for idx in range(len(x)):\n",
    "            h, w = self._compute_new_size(images.sizes[idx])\n",
    "            new_sizes.append((h, w))\n",
    "\n",
    "            # resize the positional embeddings to the new size of the image\n",
    "            emb = F.interpolate(\n",
    "                self.patch_pos_embedding[None],\n",
    "                size=(h, w),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )[0]\n",
    "\n",
    "            patch_pos_emb[idx, :, :h, :w] = emb\n",
    "\n",
    "        patch_pos_emb = patch_pos_emb.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "        class_pos_emb = self.class_pos_embedding.expand(B, 1, -1)  # (B, 1, C)\n",
    "        pos_emb = torch.cat([class_pos_emb, patch_pos_emb], dim=1)  # (B, 1+HW, C)\n",
    "\n",
    "        class_token = self.class_embedding.expand(B, 1, -1)  # (B, 1, C)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "        x = torch.cat([class_token, x], dim=1)  # (B, 1+HW, C)\n",
    "\n",
    "        x = x + pos_emb\n",
    "\n",
    "        out = Batched2DTensors(x, sizes=[(1 + h * w) for h, w in new_sizes])\n",
    "\n",
    "        return out, new_sizes\n",
    "\n",
    "    def __call__(\n",
    "        self, images: Batched3DTensors\n",
    "    ) -> tuple[Batched2DTensors, list[tuple[int, int]]]:\n",
    "        return super().__call__(images)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Encoder\n",
    "\n",
    "The text encoder used is the CLIP text encoder. Thus, differently from the vision encoder, a sentence consisting of many words is encoded into a single embedding and not into a sequence of embeddings.\n",
    "\n",
    "The only difference is that, to make the output dimension match the feature dimension of each visual patch, we change the last linear projection with a new one that reduces the dimension to 256. As for the vision encoder, we initialize the weights of this linear projection by applying PCA to the original weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from jaxtyping import Float\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import Tensor, nn\n",
    "from transformers.models.clip.modeling_clip import (\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from transformers.models.clip.processing_clip import CLIPProcessor\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"A wrapper around the CLIP [1]_ text encoder.\n",
    "\n",
    "    .. note::\n",
    "        To make the output dimension of the text encoder match the output dimension\n",
    "        of the vision encoder, if the specified `output_dim` is different from\n",
    "        the dimension of the text encoder's projection layer, the projection layer\n",
    "        is replaced with a new linear layer that has the specified output dimension.\n",
    "        The weights of the linear layer are initialized by applying PCA to the weights\n",
    "        of the original projection layer.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,\n",
    "        S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.\n",
    "        Learning transferable visual models from natural language supervision.\n",
    "        In International conference on machine learning (pp. 8748-8763). PMLR.\"\"\"\n",
    "\n",
    "    def __init__(self, model: Models, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self._dummy = nn.Parameter(torch.empty(0))\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(model.weights())\n",
    "        self.transformer = CLIPTextModelWithProjection.from_pretrained(model.weights())\n",
    "\n",
    "        projection = self.transformer.text_projection\n",
    "        if projection.out_features != output_dim:\n",
    "            weights = projection.weight.transpose(0, 1).detach().numpy()\n",
    "            weights = PCA(output_dim).fit_transform(weights)\n",
    "            self.projection = nn.Linear(\n",
    "                in_features=projection.in_features,\n",
    "                out_features=output_dim,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.projection.weight = nn.Parameter(\n",
    "                    torch.from_numpy(weights).transpose(0, 1)\n",
    "                )\n",
    "\n",
    "    def forward(self, text: list[str]) -> Float[Tensor, \"N D\"]:\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(self._dummy.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self._dummy.device)\n",
    "\n",
    "        x = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        )\n",
    "        text_embeds = x.text_embeds  # (N, D)\n",
    "        out: Tensor = self.projection(text_embeds)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __call__(self, text: list[str]) -> Float[Tensor, \"N D\"]:\n",
    "        \"\"\"Encodes each text in the batch into a vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : list[str]\n",
    "            A list of texts to encode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Float[Tensor, \"N D\"]\n",
    "            Tensor of shape (N, D) where N is the number of texts in the batch and D\n",
    "            is the output dimension of the text encoder.\n",
    "        \"\"\"\n",
    "\n",
    "        return super().__call__(text)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OwlViT\n",
    "\n",
    "As open-set object detector we decided to use OwlVit ([Minderer et al. 2022](https://arxiv.org/abs/2205.06230)) given its good performance (both in terms of inference speed and localization capabilities) and simplicity. Indeed, OwlViT is based on a pair of vision and text encoders that are pretrained on a contrastive image-text task, exactly like CLIP. Then, such architecture is finetuned on object detection by adding a simple regression head on top of the vision encoder. In particular, each patch (outputted by the vision encoder) is given in input to the regression head to predict the bounding boxes offsets with repsect to the patch position. The similarity between each patch and the text embeddings of the entities to detect is computed to obtain the confidence score that the patch contains the entity. A threshold is then applied to the confidence scores to obtain the final set of bounding boxes.\n",
    "\n",
    "Regarding the implementation, we used the one provided by [HuggingFace](https://huggingface.co/docs/transformers/model_doc/owlvit) with a modification to handle the case in which a given entity is not detected. In particular, if no instance of an entity is detected with a confidence score higher than the threshold, we select the __k__ most confident bounding boxes for that entity even if their confidence score is lower than the threshold. Even if the estimated locations of an entity are not accurate, we consider this approach better than not detecting the entity at all. Indeed, if the input scene graph is (`the girl` -- `approaching` --> `the table`), if no table is detected, the new graph will consist only of nodes associated to the entity `the girl`, thus missing the information that `the girl` that needs to be found is the one that is approaching `the table`.\n",
    "\n",
    "Other than OwlViT we also tried to use Grounding DINO as object detector. However, despite the better performance with respect to OwlViT, the inference speed was too slow making the training time for a single epoch more than double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from transformers import OwlViTForObjectDetection, OwlViTProcessor\n",
    "\n",
    "from deepsight.data.structs import (\n",
    "    Batch,\n",
    "    BoundingBoxes,\n",
    "    BoundingBoxFormat,\n",
    "    ODInput,\n",
    "    ODOutput,\n",
    ")\n",
    "\n",
    "\n",
    "class OwlViT(nn.Module):\n",
    "    \"\"\"Wrapper around the OwlViT model for open-set detection.\n",
    "\n",
    "    With respect to the original OwlViT model, this wrapper adds the possibility\n",
    "    to return the bounding boxes even when the confidence of the entity is below\n",
    "    a certain threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold: float, num_boxes: int | None = None) -> None:\n",
    "        \"\"\"Initializes the OwlViT model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        threshold : float\n",
    "            The threshold used to determine whether an entity is present in the\n",
    "            input image.\n",
    "        num_boxes : int | None\n",
    "            If not None, when an entity is not found with a confidence above the\n",
    "            `threshold`, the model will return the top `num_boxes` boxes with the\n",
    "            highest confidence scores for that entity. If None, the model will\n",
    "            return only the entities that are found with a confidence above the\n",
    "            `threshold`. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self._threshold = threshold\n",
    "        self._num_boxes = num_boxes\n",
    "\n",
    "        self._dummy = nn.Parameter(torch.empty(0))\n",
    "\n",
    "        model_id = \"google/owlvit-base-patch32\"\n",
    "        processor = OwlViTProcessor.from_pretrained(model_id)\n",
    "        model = OwlViTForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "        self.processor = processor\n",
    "        self.owlvit = model.owlvit\n",
    "        self.class_head = model.class_head\n",
    "        self.box_head = model.box_head\n",
    "\n",
    "        self.layer_norm = model.layer_norm\n",
    "\n",
    "    def _get_boxes(\n",
    "        self, image_embeds: Float[Tensor, \"B L D\"]\n",
    "    ) -> Float[Tensor, \"B L 4\"]:\n",
    "        \"\"\"Returns for each patch the corresponding bounding box.\n",
    "        \n",
    "        The bounding box associated to a patch is obtained by computing the coordinates offsets\n",
    "        with respect to the center of the patch using a simple regression head.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        image_embeds : Float[Tensor, \"B L D\"]\n",
    "            The image embeddings obtained from the OwlViT model. The shape is (B, L, D) where\n",
    "            B is the batch size, L is the number of patches and D is the dimension of the\n",
    "            embeddings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Float[Tensor, \"B L 4\"]\n",
    "            The bounding boxes associated to each patch. The shape is (B, L, 4) where B is the\n",
    "            batch size, L is the number of patches and 4 are the normalized coordinates of the bounding box\n",
    "            in the format (center_x, center_y, width, height).\n",
    "        \"\"\"\n",
    "\n",
    "        L = image_embeds.shape[1]\n",
    "        side = int(L**0.5)\n",
    "        device = image_embeds.device\n",
    "        dtype = image_embeds.dtype\n",
    "\n",
    "        coords = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(1, side + 1, device=device, dtype=dtype),\n",
    "                torch.arange(1, side + 1, device=device, dtype=dtype),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        coords = coords / side\n",
    "        coords = coords.view(L, 2)\n",
    "\n",
    "        coords = torch.clamp(coords, 0.0, 1.0)  # (L, 2)\n",
    "        coord_bias = torch.log(coords + 1e-4) - torch.log1p(-coords + 1e-4)\n",
    "\n",
    "        size = torch.full_like(coord_bias, 1.0 / side)  # (L, 2)\n",
    "        size_bias = torch.log(size + 1e-4) - torch.log1p(-size + 1e-4)\n",
    "\n",
    "        box_bias = torch.cat((coord_bias, size_bias), dim=-1)  # (L, 4)\n",
    "\n",
    "        pred_boxes: Tensor = self.box_head(image_embeds)  # (B, L, 4)\n",
    "        pred_boxes = pred_boxes + box_bias  # (B, L, 4)\n",
    "        pred_boxes = torch.sigmoid(pred_boxes)  # (B, L, 4)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "    def forward(self, inputs: Batch[ODInput]) -> Batch[ODOutput]:\n",
    "        images = [inp.image.to_pil().data for inp in inputs]\n",
    "\n",
    "        # Create list of list of entities and remove duplicates\n",
    "        entities: list[list[str]] = []\n",
    "        str_to_idx: list[dict[str, list[int]]] = []\n",
    "        for inp in inputs:\n",
    "            sample_entities = []\n",
    "            sample_str_to_idx = {}\n",
    "            for ent_idx, ent in enumerate(inp.entities):\n",
    "                # Since the OwlViT was trained by adding the prefix \"a photo of a\"\n",
    "                # to each category name, we do the same here.\n",
    "                # Indeed we find that the model is much more accurate and confident\n",
    "                # when the prefix is added.\n",
    "                ent = f\"a photo of a {ent}\"\n",
    "                if ent not in sample_str_to_idx:\n",
    "                    sample_str_to_idx[ent] = [ent_idx]\n",
    "                    sample_entities.append(ent)\n",
    "                else:\n",
    "                    sample_str_to_idx[ent].append(ent_idx)\n",
    "\n",
    "            entities.append(sample_entities)\n",
    "            str_to_idx.append(sample_str_to_idx)\n",
    "\n",
    "        B = len(images)\n",
    "        max_queries = max(len(ent) for ent in entities)\n",
    "\n",
    "        tmp = self.processor(\n",
    "            images=images, text=entities, return_tensors=\"pt\", truncation=True\n",
    "        )\n",
    "        pixel_values = tmp[\"pixel_values\"].to(self._dummy.device)\n",
    "        input_ids = tmp[\"input_ids\"].to(self._dummy.device)\n",
    "        attention_mask = tmp[\"attention_mask\"].to(self._dummy.device)\n",
    "\n",
    "        outputs = self.owlvit(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        image_tokens = outputs.vision_model_output[0]  # (B, 1+L, D)\n",
    "        image_tokens = self.owlvit.vision_model.post_layernorm(\n",
    "            image_tokens\n",
    "        )  # (B, 1+L, D)\n",
    "\n",
    "        class_token = image_tokens[:, :1, :]  # (B, 1, D)\n",
    "        image_patches = image_tokens[:, 1:, :]  # (B, L, D)\n",
    "        image_embeds = image_patches * class_token  # (B, L, D)\n",
    "        image_embeds = self.layer_norm(image_embeds)  # (B, L, D)\n",
    "\n",
    "        query_embeds = outputs[-4]  # (BQ, D) where BQ = B * max_queries\n",
    "        query_embeds = query_embeds.view(B, max_queries, -1)  # (B, Q, D)\n",
    "\n",
    "        query_mask = torch.zeros(\n",
    "            (B, max_queries),\n",
    "            dtype=torch.bool,\n",
    "            device=query_embeds.device,\n",
    "        )\n",
    "        for sample_idx, sample_entities in enumerate(entities):\n",
    "            query_mask[sample_idx, : len(sample_entities)] = True\n",
    "\n",
    "        # (B, L, Q) means that each image patch is compared to each query\n",
    "        # embedding, and the result is a scalar value representing the\n",
    "        # similarity between the two.\n",
    "        pred_logits, _ = self.class_head(image_embeds, query_embeds, query_mask)\n",
    "        pred_boxes = self._get_boxes(image_embeds)\n",
    "\n",
    "        probs, labels = pred_logits.max(dim=-1)  # (B, L)\n",
    "        if self._num_boxes is not None:\n",
    "            _, top_index_per_query = torch.topk(\n",
    "                pred_logits, self._num_boxes, dim=1\n",
    "            )  # (B, K, Q)\n",
    "\n",
    "        scores = probs.sigmoid()  # (B, L)\n",
    "\n",
    "        results = []\n",
    "        for sample_idx in range(B):\n",
    "            mask = scores[sample_idx] > self._threshold\n",
    "\n",
    "            sample_pred_boxes = pred_boxes[sample_idx, mask]  # (N, 4)\n",
    "            sample_pred_labels = labels[sample_idx, mask]  # (N,)\n",
    "            sample_pred_scores = scores[sample_idx, mask]  # (N,)\n",
    "\n",
    "            boxes_list: list[Tensor] = []\n",
    "            labels_list: list[int] = []\n",
    "            scores_list: list[Tensor] = []\n",
    "\n",
    "            for ent_idx, ent in enumerate(entities[sample_idx]):\n",
    "                indices = sample_pred_labels == ent_idx\n",
    "                num_found = indices.sum().item()\n",
    "                if num_found > 0:\n",
    "                    # entity found in image\n",
    "                    # add all duplicates to the list\n",
    "                    for j in str_to_idx[sample_idx][ent]:\n",
    "                        boxes_list.append(sample_pred_boxes[indices])\n",
    "                        labels_list.extend([j] * num_found)\n",
    "                        scores_list.extend(sample_pred_scores[indices])\n",
    "                elif self._num_boxes is not None:\n",
    "                    # entity not found in image\n",
    "                    # add top K boxes for the entity\n",
    "                    topk_boxes = pred_boxes[\n",
    "                        sample_idx, top_index_per_query[sample_idx, :, ent_idx]\n",
    "                    ]\n",
    "\n",
    "                    topk_scores = scores[\n",
    "                        sample_idx, top_index_per_query[sample_idx, :, ent_idx]\n",
    "                    ]\n",
    "\n",
    "                    for j in str_to_idx[sample_idx][ent]:\n",
    "                        boxes_list.append(topk_boxes)\n",
    "                        labels_list.extend([j] * self._num_boxes)\n",
    "                        scores_list.extend(topk_scores * self._num_boxes)\n",
    "\n",
    "            sample_labels = torch.tensor(\n",
    "                labels_list, dtype=torch.long, device=self._dummy.device\n",
    "            )\n",
    "\n",
    "            boxes = BoundingBoxes(\n",
    "                tensor=torch.cat(boxes_list, dim=0),\n",
    "                images_size=inputs[sample_idx].image.size,\n",
    "                format=BoundingBoxFormat.CXCYWH,\n",
    "                normalized=True,\n",
    "            )\n",
    "\n",
    "            sample_scores = torch.cat(scores_list, dim=0)\n",
    "\n",
    "            results.append(\n",
    "                ODOutput(\n",
    "                    boxes=boxes,\n",
    "                    entities=sample_labels,\n",
    "                    scores=sample_scores,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Batch(results)\n",
    "\n",
    "    def __call__(self, inputs: Batch[ODInput]) -> Batch[ODOutput]:\n",
    "        \"\"\"Given a batch of images and entities to be detected, returns a list\n",
    "        of OSDOutput objects containing the bounding boxes of the detected entities.\n",
    "\n",
    "        .. note::\n",
    "            In case of duplicate entities for the same image, this implementation will\n",
    "            return the same bounding boxes for both entities. This is different from the\n",
    "            implementation of HuggingFace's OwlViT model which returns the bounding\n",
    "            boxes only for one of the entities (usually the first one).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Batch[OSDInput]\n",
    "            A Batch object containing OSDInput objects containing the images and\n",
    "            entities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Batch[OSDOutput]\n",
    "            A Batch object containing the OSDOutput objects containing the bounding\n",
    "            boxes.\n",
    "        \"\"\"\n",
    "\n",
    "        return super().__call__(inputs)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criterion\n",
    "\n",
    "The criterion is responsible for the loss calculation. To calculate the loss we first need to choose which of the candidate bounding boxes to associate to the ground truth bounding box. As previously said, we only consider the bounding boxes obtained from the graph nodes that correspond to the entity that is the subject of the region description, sine this is the target entity to detect. Notice that here we make the assumption that the first entity of the scene graph is the subject of the region description. We deemed not necessary to apply a NLP tool to extract the subject of the sentence since we never observed a case in the dataset where the subject was not the first entity.\n",
    "\n",
    "To decide which of the candidate bounding boxes match to the target bounding box, we adopt the same approach used by DETR-like models. In particular, for each candidate bounding box, we compute the cost of matching it with the ground truth. The cost is computed as the weighted sum of three differents factors:\n",
    "1. the L1 distance between the coordinates of the candidate bounding box and the ground truth bounding box;\n",
    "2. the Generalized IoU between the candidate bounding box and the ground truth bounding box;\n",
    "3. the negative cosine similarity between the node embedding of the candidate bounding box and the text embedding of the region description.\n",
    "Notice that since there is only one ground truth bounding box, we do not need to apply the full Hungarian matching algorithm, but we can simple select the candidate that minimizes the cost.\n",
    "\n",
    "Once the matching is done, we can compute the loss between the matched bounding boxes. The loss is computed as the weighted sum of three different losses:\n",
    "1. the L1 loss between the coordinates of the matched bounding boxes;\n",
    "2. the Generalized IoU ([Rezatofighi et al. 2019](https://arxiv.org/abs/1902.09630)) loss between the matched bounding boxes;\n",
    "3. the InfoNCE loss where the positive sample is the embedding of the matched node and the negative samples are the embeddings of the other nodes.\n",
    "\n",
    "The first two losses are the canonical losses used by DETR-like models to make the network learn to correctly predict the bounding box coordinates. Here, we remove the classification loss (usually computed using focal loss) since we do not need to predict the class of the selected node; its class is already known from the input scene graph. However, since at inference time we select among the candidate nodes the one with the highest similarity with the text embedding of the region description, we use a contrastive loss (here, InfoNCE) to force the network to pull together the embeddings of the matched node and the text embedding of the region description and to push away the embeddings of the other wrong nodes. Since it has been shown that InfoNCE performs better when the number of negative samples is high, as negative samples we do not use only the other subject nodes but all the nodes of all the graphs in the same batch.\n",
    "\n",
    "Finally, similarly to other DETR-like models, the loss is computed not only with respect to the output of the last decoder layer but also with respect to the output of the intermediate decoder layers. In particular, for each layer we recompute the matching and the corresponding loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from deepsight.data.structs import Batch, BoundingBoxes, RECOutput\n",
    "from deepsight.measures import Loss, Reduction\n",
    "from deepsight.measures.losses import BoxL1Loss, GeneralizedBoxIoULoss, InfoNCELoss\n",
    "from deepsight.modeling.pipeline import Criterion as _Criterion\n",
    "\n",
    "from projects.sgg.modeling._config import CriterionConfig\n",
    "from projects.sgg.modeling._structs import ModelOutput\n",
    "\n",
    "\n",
    "class Criterion(_Criterion[ModelOutput, RECOutput]):\n",
    "    def __init__(self, config: CriterionConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.auxiliary = config.auxiliary\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "        self.l1_cost = config.l1_cost\n",
    "        self.giou_cost = config.giou_cost\n",
    "        self.similarity_cost = config.similarity_cost\n",
    "\n",
    "        self.l1_weight = config.l1_weight\n",
    "        self.giou_weight = config.giou_weight\n",
    "        self.infonce_weight = config.infonce_weight\n",
    "\n",
    "        self.l1_loss = BoxL1Loss(reduction=Reduction.NONE)\n",
    "        self.giou_loss = GeneralizedBoxIoULoss(reduction=Reduction.NONE)\n",
    "        self.infonce_loss = InfoNCELoss(\n",
    "            temperature=config.temperature, reduction=Reduction.MEAN\n",
    "        )\n",
    "\n",
    "    def losses_names(self) -> list[str]:\n",
    "        losses = []\n",
    "        if self.auxiliary:\n",
    "            losses += [f\"L1_{i}\" for i in range(self.num_layers)]\n",
    "            losses += [f\"GIoU_{i}\" for i in range(self.num_layers)]\n",
    "            losses += [f\"InfoNCE_{i}\" for i in range(self.num_layers)]\n",
    "        else:\n",
    "            losses += [\"L1\", \"GIoU\", \"InfoNCE\"]\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _compute_layer_loss(\n",
    "        self,\n",
    "        output: ModelOutput,\n",
    "        tgt_boxes: BoundingBoxes,\n",
    "        layer_idx: int,\n",
    "    ) -> list[Loss]:\n",
    "        \"\"\"Computes the loss for the output of a single layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : ModelOutput\n",
    "            The output of the model.\n",
    "        tgt_boxes : BoundingBoxes\n",
    "            The target boxes. The tensor has shape (B, N, 4).\n",
    "        layer_idx : int\n",
    "            The index of the layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Loss]\n",
    "            A list of the computed losses.\n",
    "        \"\"\"\n",
    "\n",
    "        B, N = output.padded_entities.shape\n",
    "        subject_mask = output.padded_entities != 0\n",
    "        padding_mask = output.padded_entities == N\n",
    "\n",
    "        out_boxes = output.boxes[layer_idx].to_cxcywh().normalize()  # (B, N, 4)\n",
    "\n",
    "        l1_loss = self.l1_loss(out_boxes, tgt_boxes)  # (B, N)\n",
    "        giou_loss = self.giou_loss(out_boxes, tgt_boxes)  # (B, N)\n",
    "\n",
    "        nodes = output.graphs[layer_idx].nodes(pad_value=0.0)  # (B, N, D)\n",
    "        captions = output.captions.unsqueeze(1).expand(-1, N, -1)  # (B, N, D)\n",
    "        similarity = torch.cosine_similarity(nodes, captions, dim=-1)  # (B, N)\n",
    "\n",
    "        cost = (\n",
    "            self.l1_cost * l1_loss\n",
    "            + self.giou_cost * giou_loss\n",
    "            - self.similarity_cost * similarity\n",
    "        )\n",
    "\n",
    "        cost = cost.masked_fill_(subject_mask, torch.inf)  # (B, N)\n",
    "        idx = cost.min(dim=1)[1]  # (B,)\n",
    "\n",
    "        pos_mask = torch.zeros_like(output.padded_entities, dtype=torch.bool)  # (B, N)\n",
    "        pos_mask[torch.arange(B), idx] = True\n",
    "\n",
    "        nodes = output.graphs[layer_idx].nodes(pad_value=0.0)  # (B, N, D)\n",
    "        queries = output.captions  # (B, D)\n",
    "        pos_keys = nodes[pos_mask]\n",
    "        neg_mask = torch.logical_xor(pos_mask, ~padding_mask)\n",
    "        neg_keys = nodes[neg_mask]\n",
    "        infonce_loss = self.infonce_loss(queries, pos_keys, neg_keys)  # (B,)\n",
    "\n",
    "        l1_loss = l1_loss[pos_mask].mean()\n",
    "        giou_loss = giou_loss[pos_mask].mean()\n",
    "\n",
    "        if layer_idx == -1:\n",
    "            return [\n",
    "                Loss(\"L1\", l1_loss, self.l1_weight),\n",
    "                Loss(\"GIoU\", giou_loss, self.giou_weight),\n",
    "                Loss(\"InfoNCE\", infonce_loss, self.infonce_weight),\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                Loss(f\"L1_{layer_idx}\", l1_loss, self.l1_weight),\n",
    "                Loss(f\"GIoU_{layer_idx}\", giou_loss, self.giou_weight),\n",
    "                Loss(f\"InfoNCE_{layer_idx}\", infonce_loss, self.infonce_weight),\n",
    "            ]\n",
    "\n",
    "    def forward(self, output: ModelOutput, targets: Batch[RECOutput]) -> list[Loss]:\n",
    "        B, N = output.padded_entities.shape\n",
    "\n",
    "        tgt_boxes = BoundingBoxes.stack([tgt.box for tgt in targets], dim=0)  # (B, 4)\n",
    "        tgt_boxes = tgt_boxes.to_cxcywh().normalize()  # (B, 4)\n",
    "        tgt_boxes = tgt_boxes.unsqueeze(1).expand(-1, N, -1)  # (B, N, 4)\n",
    "\n",
    "        if self.auxiliary:\n",
    "            losses = []\n",
    "            for i in range(self.num_layers):\n",
    "                losses += self._compute_layer_loss(output, tgt_boxes, i)\n",
    "        else:\n",
    "            losses = self._compute_layer_loss(output, tgt_boxes, -1)\n",
    "\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PostProcessor\n",
    "\n",
    "As previously described, to obtain the candidate bounding box, from the graph outputted by the last decoder layer, we select the nodes that refer to the subject of the region description. Then, the similarity between the node embeddings and the text embedding of the description is computed using cosine similarity. The bounding box associated to the nodes with the highest similarity is then returned as the candidate bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from deepsight.data.structs import Batch, RECOutput\n",
    "from deepsight.modeling.pipeline import PostProcessor as _PostProcessor\n",
    "\n",
    "from projects.sgg.modeling._structs import ModelOutput\n",
    "\n",
    "\n",
    "class PostProcessor(_PostProcessor[ModelOutput, RECOutput]):\n",
    "    def forward(self, output: ModelOutput) -> Batch[RECOutput]:\n",
    "        B, N = output.padded_entities.shape\n",
    "        subject_mask = output.padded_entities != 0  # (B, N)\n",
    "\n",
    "        queries = output.captions.unsqueeze(1)  # (B, 1, D)\n",
    "        keys = output.graphs[-1].nodes(pad_value=0.0)  # (B, N, D)\n",
    "\n",
    "        similarity = torch.cosine_similarity(queries, keys, dim=-1)  # (B, N)\n",
    "        similarity.masked_fill_(subject_mask, -torch.inf)  # (B, N)\n",
    "\n",
    "        idx = similarity.max(dim=1)[1]  # (B,)\n",
    "\n",
    "        boxes = output.boxes[-1][torch.arange(B), idx]  # (B, 4)\n",
    "\n",
    "        return Batch([RECOutput(box=boxes[i]) for i in range(B)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
