{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAI2DcQfOZD"
      },
      "source": [
        "# Visgator\n",
        "[![pdm-managed](https://img.shields.io/badge/pdm-managed-blueviolet)](https://pdm.fming.dev)\n",
        "\n",
        "Referring expressions visual grounding wih scene graphs and object detection.\n",
        "This repo includes early-stage work from [visgator-base](https://github.com/halixness/visgator-base).\n",
        "\n",
        "### Authors\n",
        "Diego Calanzone, Francesco Gentile. <br>\n",
        "University of Trento <br>\n",
        "Deep Learning, Capstone Project, Spring 2023."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A note on code\n",
        "The codebase for this project is articulated. We have developed a Python module with an object-oriented approach to practically train and test our model on multiple machines and to build a template code for Deep Learning projects in the future. This report will contain comprehensive textual descriptions with annotated code snippets for all the source files were code has been written (class templates excluded). <br>\n",
        "Moreover, all the executable cells in sequence will allow to test all the experimental steps from code generation to model evaluation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "Object detection with referring expression is a sub-case of a major computer vision challenge: associating semantics with identified portions of the image. Recent approaches in deep computer vision are based on learning without annotations to leverage a massive amount of internet-scraped data. OpenAI CLIP ([Radford et al.](https://arxiv.org/pdf/2103.00020.pdf)) implements a visual and a language backbone to encode images and captions, respectively, and match their representation vectors in a common embedding space; by providing a score for a caption to match an image, this model can be used for open-set classification and object detection. This contrasts with a less recent branch in object detection: closed-set detectors. YOLO ([Redmon et al.](https://arxiv.org/abs/1506.02640)) is a widely-used object detector that classifies portions of images with a finite set of labels. <br> \n",
        "Grounding DINO ([Liu et al.](https://arxiv.org/abs/2303.05499)) proposes to extend object detection from closed to open sets by fusing language and visual modalities in multiple stages: after a feature enhancing layer, visual features, considered as query tokens, are selected with respect to language feature tokens; consequently, language (K,V) and vision features(Q) attend each other through cross-attention; eventually, entities in the language modality are matched with the output visual queries with a contrastive objective. <br>\n",
        "Similarly, M-Plug 2 ([Xu et al.](https://arxiv.org/pdf/2302.00402.pdf)) proposes an architecture consisting in shared self/cross attention layers (universal layers), a cross-attention fusion model and a shared weights decoder block. In both of these methods, the proposed model is massively pre-trained on a set of datasets and object detection with referring expression is one of a multitude of target tasks. <br>\n",
        "<br>\n",
        "Given the sparsity of such pre-trained models with different capabilities, the goal of this project is to follow training efficiency by combining pre-trained visual and language backbones: a minimal portion of the proposed architecture is trained from scratch to \"glue\" the intermediate outputs of different specialized models. In particular:\n",
        "1. We want to leverage the grounded text and image representations from CLIP, which has already been trained on massive datasets.\n",
        "2. We introduce some \"bias\" by delegating the tasks of identifying entities in images and captions to existing pre-trained models (GDino, OwLViT, YOLO, SceneGraph Parser ([Schuster et al.](https://nlp.stanford.edu/software/scenegraph-parser.shtml))). This saves us the burden of training a massive network from scratch to learn these skills, while it allows us to test and leverage existing work from literature.\n",
        "3. We introduce two attention-based modules trained from scratch, in order: a transformer decoder to ground visual entity pairs with referring expressions; a transformer encoder to compare language-informed entity pairs and extract the correct one as referred from annotations."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline\n",
        "### Architecture\n",
        "We choose as baseline model \"YOLOCLIP\": a simple architecture consisting in a detection module, [YOLOv8 (Ultralytics)](https://docs.ultralytics.com); a language-image pairing module, [OpenAI CLIP (HuggingFace)](https://huggingface.co/docs/transformers/model_doc/clip). For each `(image, caption)` pair: \n",
        "1. Given an image, the bounding boxes are generated with YOLO ([Redmon et al.](https://arxiv.org/abs/1506.02640)) for all the detected entities.\n",
        "2. Each portion of image denoted by each bounding box is preprocessed with CLIP (resizing, center crop, RGB conversion, normalization) and encoded with the visual backbone (`ViT-B/32`) into a sequence of patch token embeddings.\n",
        "3. The image caption is tokenized with [CLIP's BytePair Encoding](https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py) and encoded with CLIP's language backbone into a sequence of token embeddings.\n",
        "4. Both the image and language token embeddings are mapped into a visual and textual vector embedding, respectively, with same dimensionality $d = 512$. For each pair of `(caption, bbox_img_embedding)` the matching score is computed as follows (section 2.4, [Radford et al.](https://arxiv.org/pdf/2103.00020.pdf)):\n",
        "<br><br>\n",
        "```python\n",
        "    # image_encoder - ResNet or Vision Transformer\n",
        "    # text_encoder - CBOW or Text Transformer\n",
        "    # I[n, h, w, c] - minibatch of aligned images\n",
        "    # T[n, l] - minibatch of aligned texts\n",
        "    # W_i[d_i, d_e] - learned proj of image to embed\n",
        "    # W_t[d_t, d_e] - learned proj of text to embed\n",
        "    # t - learned temperature parameter\n",
        "    # extract feature representations of each modality\n",
        "    I_f = image_encoder(I) #[n, d_i]\n",
        "    T_f = text_encoder(T) #[n, d_t]\n",
        "    # joint multimodal embedding [n, d_e]\n",
        "    I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
        "    T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
        "    # scaled pairwise cosine similarities [n, n]\n",
        "    logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
        "    # symmetric loss function\n",
        "    labels = np.arange(n)\n",
        "    loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
        "    loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
        "    loss = (loss_i + loss_t)/2\n",
        "```\n",
        "5. From the pair `(caption, bbox_img_embedding)` with the highest score, the bounding box is considered as the prediction for the given referring expression."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation\n",
        "We implement the baseline model in `src/visgator/models/baseline/_model.py`:\n",
        "<br><br>\n",
        "```python\n",
        "class Model(_Model[BBoxes]):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._postprocessor = PostProcessor()\n",
        "\n",
        "        self._yolo = YOLO(config.yolo.weights())\n",
        "        self._clip_processor = CLIPProcessor.from_pretrained(config.clip.weights())\n",
        "        self._clip = CLIPModel.from_pretrained(config.clip.weights())\n",
        "\n",
        "        self._toPIL = T.ToPILImage()\n",
        "    (...)\n",
        "\n",
        "    def forward(self, batch: Batch) -> BBoxes:\n",
        "        boxes = []\n",
        "\n",
        "        # Preprocessing for YOLO\n",
        "        images = [self._toPIL(sample.image) for sample in batch.samples]\n",
        "        yolo_results = self._yolo.predict(images, conf=0.5, verbose=False)\n",
        "\n",
        "        sample: BatchSample\n",
        "        for sample, result in zip(batch, yolo_results):\n",
        "            proposals = []\n",
        "\n",
        "            # Base case: no entities detected: suppose the whole image as context\n",
        "            if len(result.boxes) == 0:\n",
        "                # create a dummy bbox\n",
        "                tmp = BBox((0, 0, 0, 0), sample.image.shape[1:], BBoxFormat.XYXY, True)\n",
        "                boxes.append(tmp.to(self._clip.device))\n",
        "                continue\n",
        "\n",
        "            # Each bounding box is a potion of img to encode with CLIP\n",
        "            for bbox in result.boxes:\n",
        "                xmin, ymin, xmax, ymax = bbox.xyxy.int()[0]\n",
        "                clipped_image = sample.image[:, ymin:ymax, xmin:xmax]\n",
        "                proposals.append(clipped_image)\n",
        "\n",
        "            inputs = self._clip_processor(\n",
        "                text=sample.caption.sentence,\n",
        "                images=proposals,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(self._clip.device)\n",
        "\n",
        "            # Compute the CLIP scores and take the bbox with the highest score\n",
        "            output = self._clip(**inputs)\n",
        "            best = output.logits_per_image.argmax(0).item()\n",
        "            boxes.append(\n",
        "                BBox(\n",
        "                    result.boxes[best].xyxy[0],\n",
        "                    sample.image.shape[1:],\n",
        "                    BBoxFormat.XYXY,\n",
        "                    False,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return BBoxes.from_bboxes(boxes)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proposed solution\n",
        "### Architecture\n",
        "![visgator architecture](https://github.com/FrancescoGentile/visgator/blob/main/docs/src/architecture.png?raw=true)\n",
        "\n",
        "We call the proposed architecture **ERP-A: Entity Relationship Pairs Attention**. <br>\n",
        "Referring expressions are informative to identify objects in two ways: by adding details about a specific object (e.g. the girl with the green jacket); by describing a relationship between multiple objects (e.g. the glass on the table). To achieve this we leverage multiple existing techniques: the Stanford SceneGraph parser to extract pairwise entity relationships from captions; OpenAI's CLIP to encode the image and the caption with significant representations from contrastive pre-training; OwLViT/Grounding DINO only to detect the entity nouns from the SceneGraph in the image. <br>\n",
        "Formally, for each `(image, caption)` sample:\n",
        "1. We identify all the Entity-Relationship-Pairs (ERP) from the caption ([Preprocessing](#preprocessing))\n",
        "2. Given multiple portions of the image as detected candidates for each entity, we instantiate an ERP for all the possible pairs of visual entities, limited with a confidence threshold ([Instantiation](#instantiation)).\n",
        "3. Enrich each entity-pair (a sequence of visual tokens) with the encoded referring expression through cross-attention ([Decoding](#decoding)).\n",
        "4. Attend to all the ERPs (each is a sequence of text-informed visual tokens) with a learnable regression token and project this to predict the proposed visual region of interest ([ERP-Attention](#erpattention))."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Preprocessing\n",
        "### Scene graph extraction\n",
        "Multiple techniques have been tested to extract entities and relationships from text:\n",
        "- <ins>GPT models</ins>: we have designed a prompt to have GPT 3.5/GPT 4 extract (ent1, rel, ent2) tuples.\n",
        "```\n",
        "    Consider the sentence: {sentence}.\n",
        "    What are the named entities?\n",
        "    What are the relations between the named entities?\n",
        "    Answer only with tuples \"[x, action name, y]\"  and without passive forms.\n",
        "    Please be coherent with the name of the actions that occur multiple times.\n",
        "    Answer by filling a JSON, follow the example:\n",
        "    Sentence: \"the girl is looking at the table full of drinks\"\n",
        "    Answer:\n",
        "    {{\n",
        "    \"entities\": [\"the girl\", \"the table\", \"drinks\"],\n",
        "    \"relations\": [[0, \"is on\", 1], [1, \"full of\", 2]]\n",
        "    }}\n",
        "```\n",
        "We attempted to identify the most difficult language expressions and to provide them as few-shot examples. The model generated valid outputs for the simplest captions:\n",
        "```\n",
        "Sentence: \"the girl is looking at the table full of drinks\"\n",
        "Answer: {\n",
        "    \"entities\": [\"the girl\", \"the table full of drinks\"],\n",
        "    \"relations\": [[0, \"is looking at\", 1]]\n",
        "}\n",
        "```\n",
        "While it showed inconsistent behavior with incomplete/grammarly incorrect/more abstract descriptions:\n",
        "```\n",
        "Sentence: \"the leftmost dog\"\n",
        "Answer: { \n",
        "    \"entities\": [\"the leftmost dog\"], \n",
        "    \"relations\": [] \n",
        "}\n",
        "```\n",
        "Moreover, the inference overhead is significant: a single query to the GPT APIs takes around $0.5 \\pm 1s$, this sums up to the overall model inference time of around $0.8 \\pm 1s$, resulting in circa $1.3s$. For for around 62.000 training examples, this would result in 22 hours per epoch, which exceeds our capabilities. We have tested multiple backends: [YOU](you.com), [OpenAI](chat.openai.com), [Poe](poe.com); no significant differences have been found.\n",
        "\n",
        "- <ins>Pre-trained Quantized LLMs</ins>: we have tested the same approach with large language models loaded locally on our machines. In order to use models of 7-13 billion of parameters on single gpus (8/16GB of VRAM), we have used quantized LLMs with weights cast to `float8` (8 bit quantized), which yields acceptable perplexity scores. We have tested a variety of models: [WizardLM-Vicuna 13B GGML](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML), [Falcon-Instruct 7B GPTQ](https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ). No significant gains, apart from inference, have been observed wrt. GPT API models; conversely, these models generated much more inconsistent outputs, possibly because of the limited capacity of these models.  \n",
        "\n",
        "- <ins>Stanford SceneGraph Parser</ins>: it consists in a rule-based or a classifier-based model to extract entities from a sentence. The latter is shown to perform better ([Schuster et al.](https://nlp.stanford.edu/pubs/schuster-krishna-chang-feifei-manning-vl15.pdf)). This model yields acceptable graph representations of the relationships described in RefCOCOg's sentences: only `281/62176` captions resulted in zero entities identified, these samples have been neglected and the implications on accuracy are reported subsequently.\n",
        "\n",
        "### Implementation\n",
        "We use a modified version of the Stanford SceneGraph Parser [built with sPaCy](https://github.com/vacancy/SceneGraphParser).\n",
        "The dataset is initially pre-processed to process captions into [SceneGraphs](https://en.wikipedia.org/wiki/Scene_graph). With `visgator.datasets.refcocog._generator.Generator.generate()`, each image annotation is parsed through a `visgator.utils.graph.SpacySceneGraphParser` ([reference](https://github.com/vacancy/SceneGraphParser)) or a Large Language Model ([reference](https://huggingface.co/tiiuae/falcon-7b-instruct)): entities and relationships between them are identified and stored in a graph object. The generator encodes each dataset sample as a tuple of `(image_path, caption, graph)`.\n",
        "\n",
        "The caption generation process can initiated with:\n",
        "```python -m visgator --phase generate --config config/local.yaml```\n",
        "\n",
        "Which is the equivalent of the executable code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ruamel.yaml import YAML\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "cfg_path = os.path.join(\"..\", \"config\", \"local.yaml\")\n",
        "config_path = Path(cfg_path)\n",
        "extention = config_path.suffix\n",
        "match extention:\n",
        "    case \".json\":\n",
        "        with open(config_path, \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "    case \".yaml\":\n",
        "        yaml = YAML(typ=\"safe\")\n",
        "        cfg = yaml.load(config_path)\n",
        "    case _:\n",
        "        raise ValueError(f\"Unknown config file extention: {extention}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from visgator.datasets import Config as DatasetConfig\n",
        "from visgator.datasets import Generator\n",
        "\n",
        "dataset_config = DatasetConfig.from_dict(cfg[\"dataset\"])\n",
        "generator = Generator.new(dataset_config)\n",
        "generator.generate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which corresponds to the generation function:\n",
        "<br><br>\n",
        "```python\n",
        "def generate(self) -> None:\n",
        "  split_samples = get_preprocessed_samples(\n",
        "      self._config,\n",
        "      [Split.TRAIN, Split.VALIDATION, Split.TEST],\n",
        "  )\n",
        "\n",
        "  parser = SceneGraphParser.new(self._config.generation.parser)\n",
        "  if self._config.generation.num_workers is not None:\n",
        "      num_workers = self._config.generation.num_workers\n",
        "  else:\n",
        "      num_workers = multiprocessing.cpu_count() // 2\n",
        "\n",
        "  output: dict[str, list] = {}  # type: ignore\n",
        "  for split, samples in split_samples.items():\n",
        "      split_output: list[dict] = []  # type: ignore\n",
        "      output[str(split)] = split_output\n",
        "\n",
        "      graphs = process_map(\n",
        "          parser.parse,\n",
        "          (sample.caption.sentence for sample in samples),\n",
        "          max_workers=num_workers,\n",
        "          chunksize=self._config.generation.chunksize,\n",
        "          desc=f\"Generating {split} split\",\n",
        "          total=len(samples),\n",
        "      )\n",
        "\n",
        "      output[str(split)] = [\n",
        "          {\n",
        "              \"image\": sample.path.name,\n",
        "              \"caption\": Caption(sample.caption.sentence, graph).to_dict(),\n",
        "              \"bbox\": sample.bbox,\n",
        "          }\n",
        "          for sample, graph in zip(samples, graphs)\n",
        "      ]\n",
        "\n",
        "  output_path = (\n",
        "      self._config.path / f\"annotations/info_{self._config.split_provider}.json\"\n",
        "  )\n",
        "  with open(output_path, \"w\") as f:\n",
        "      json.dump(output, f, indent=2)\n",
        "```\n",
        "<br><br>\n",
        "The generated annotations file for RefCOCOg in UMD format:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"test\": [\n",
        "    {\n",
        "      \"image\": \"COCO_train2014_000000380440.jpg\",\n",
        "      \"caption\": {\n",
        "        \"sentence\": \"the man in yellow coat\",\n",
        "        \"graph\": {\n",
        "          \"entities\": [{\"span\": \"the man\", \"head\": \"man\"}, ...],\n",
        "          \"relations\": [{\n",
        "              \"subject\": 0,\n",
        "              \"predicate\": \"in\",\n",
        "              \"object\": 1\n",
        "            }],\n",
        "        }\n",
        "      },\n",
        "      \"bbox\": [374.31,65.06,136.04,201.94]\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training logic\n",
        "\n",
        "Training starts with the instatiation of a `visgator.engines.Trainer` object, followed by `Trainer.run()`. <br>\n",
        "Under the hood, for `num_epochs` the method `Trainer._train_epoch(epoch)` is invoked:\n",
        "<br><br>\n",
        "```python\n",
        "def _train_epoch(self, epoch: int) -> None:\n",
        "    self._logger.info(f\"Training epoch {epoch + 1} started.\")\n",
        "\n",
        "    start = timer()\n",
        "\n",
        "    self._model.train()\n",
        "    self._postprocessor.train()\n",
        "    self._criterion.train()\n",
        "\n",
        "    self._tl_tracker.increment()\n",
        "    self._tm_tracker.increment()\n",
        "    self._optimizer.zero_grad()\n",
        "\n",
        "    counter = tqdm(\n",
        "        desc=\"Training\",\n",
        "        total=self._get_steps_per_epoch() * self._params.train_batch_size,\n",
        "    )\n",
        "\n",
        "    with counter as progress_bar:\n",
        "        batch: Batch\n",
        "        bboxes: BBoxes\n",
        "        device_type = \"cuda\" if self._device.is_cuda else \"cpu\"\n",
        "\n",
        "        for idx, (batch, bboxes) in enumerate(self._train_loader):\n",
        "            # since the dataloader batch size is equal to true batch size\n",
        "            # // gradient accumulation steps, the last samples in the dataloader\n",
        "            # may not be enough to fill the true batch size, so we break the loop\n",
        "            # for example, if the true batch size is 8, the gradient accumulation\n",
        "            # steps is 4 and the dataset size is 50, the last 2 samples will be\n",
        "            # ignored\n",
        "            if progress_bar.total == progress_bar.n:\n",
        "                break\n",
        "\n",
        "            batch = batch.to(self._device.to_torch())\n",
        "            bboxes = bboxes.to(self._device.to_torch())\n",
        "\n",
        "            # Mixed precision training support\n",
        "            with autocast(device_type, enabled=self._params.mixed_precision):\n",
        "                outputs = self._model(batch)\n",
        "                tmp_losses = self._criterion(outputs, bboxes)\n",
        "                losses = self._tl_tracker(tmp_losses)\n",
        "\n",
        "                # Gradient accumulation allows for larger batch sizes with memory limitations!\n",
        "                loss = losses.total / self._params.gradient_accumulation_steps\n",
        "\n",
        "            # GradScaling to prevent gradient underflow\n",
        "            self._scaler.scale(loss).backward()\n",
        "\n",
        "            if (idx + 1) % self._params.gradient_accumulation_steps == 0:\n",
        "                if self._params.max_grad_norm is not None:\n",
        "                    self._scaler.unscale_(self._optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(  # type: ignore\n",
        "                        self._model.parameters(), self._params.max_grad_norm\n",
        "                    )\n",
        "\n",
        "                self._scaler.step(self._optimizer)\n",
        "                self._scaler.update()\n",
        "                self._optimizer.zero_grad()\n",
        "                self._lr_scheduler.step_after_batch()\n",
        "                \n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_bboxes = self._postprocessor(outputs)\n",
        "                self._tm_tracker.update(\n",
        "                    pred_bboxes.to_xyxy().normalize().tensor,\n",
        "                    bboxes.to_xyxy().normalize().tensor,\n",
        "                )\n",
        "\n",
        "            progress_bar.update(len(batch))\n",
        "            \n",
        "            del batch\n",
        "            del bboxes\n",
        "            del outputs\n",
        "            del loss\n",
        "\n",
        "            # Memory cleanup to prevent leaking\n",
        "            if self._device.is_cuda:\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "    # By default the LR is rescaled after each epoch (1e-3 => 1e-4 => ...)\n",
        "    self._lr_scheduler.step_after_epoch()\n",
        "\n",
        "    end = timer()\n",
        "    elapsed = end - start\n",
        "\n",
        "    self._logger.info(f\"Training epoch {epoch + 1} finished.\")\n",
        "    self._log_statistics(epoch, elapsed, train=True)\n",
        "```\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>Training batch size</td>\n",
        "        <td>32</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Evaluation batch size</td>\n",
        "        <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Gradient accumulation step</td>\n",
        "        <td>8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Start learning rate</td>\n",
        "        <td>1e-3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>LR Scheduler</td>\n",
        "        <td><a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\">OneCycleLR</a></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>LR scheduler interval</td>\n",
        "        <td>1 epoch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Max LR</td>\n",
        "        <td>1e-3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Optimizer</td>\n",
        "        <td>AdamW</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Box confidence threshold</td>\n",
        "        <td>0.1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Text confidence threshold</td>\n",
        "        <td>0.25</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Max detected boxes</td>\n",
        "        <td>50</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Hidden dim</td>\n",
        "        <td>256</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch formatting\n",
        "\n",
        "Samples from the RefCOCOg are organized in a sequence of `Batch` objects, containing `BatchSample` samples.<br> \n",
        "A `BatchSample` object consists in an `image:tensor[B,W,HW,]` and a `caption:Caption`.<br>\n",
        "A `Caption` object encapsulates the original text description and the parsed `SceneGraph`. \n",
        "<br><br>\n",
        "```python\n",
        "    @serde.serde(type_check=serde.Strict)\n",
        "    @dataclass(frozen=True)\n",
        "    class Caption:\n",
        "        \"\"\"A caption with an optional scene graph.\"\"\"\n",
        "        sentence: str\n",
        "        graph: Optional[SceneGraph] = serde.field(\n",
        "            default=None,\n",
        "            serializer=SceneGraph.to_dict,\n",
        "            deserializer=SceneGraph.from_dict,\n",
        "        )\n",
        "        (...)\n",
        "\n",
        "\n",
        "    @dataclass(frozen=True)\n",
        "    class BatchSample:\n",
        "        \"\"\"A batch sample with an image and a caption.\"\"\"\n",
        "        image: UInt8[Tensor, \"3 H W\"]\n",
        "        caption: Caption\n",
        "        (...)\n",
        "\n",
        "\n",
        "    @dataclass(frozen=True)\n",
        "    class Batch:\n",
        "        \"\"\"A batch of samples.\"\"\"\n",
        "        samples: tuple[BatchSample, ...]\n",
        "        (...)\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture\n",
        "Our model, renamed in the configuration as `erpa` is invoked in `visgator.engines.trainer.Trainer.train_epoch()` and it is composed by 4 blocks:\n",
        "\n",
        "### 1. Encoders\n",
        "We use the [OpenCLIP](https://github.com/mlfoundations/open_clip) implementation for CLIP's visual and textual backbone. The source has been copied in `src/visgator/models/erpa/_encoders.py`:\n",
        "\n",
        "```python\n",
        "\n",
        "def build_encoders(config: EncodersConfig) -> tuple[VisionEncoder, TextEncoder]:\n",
        "    \"\"\" Returns the vision and text backbone from OpenCLIP\"\"\"\n",
        "\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "        config.model,\n",
        "        pretrained=config.pretrained,\n",
        "    )\n",
        "\n",
        "    tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "\n",
        "    if type(preprocess) is not T.Compose:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Image normalization as preprocessing pipeline for the vision encoder\n",
        "    mean: Optional[tuple[float, float, float]] = None\n",
        "    std: Optional[tuple[float, float, float]] = None\n",
        "\n",
        "    for transform in preprocess.transforms:\n",
        "        if type(transform) is T.Normalize:\n",
        "            mean = transform.mean\n",
        "            std = transform.std\n",
        "            break\n",
        "\n",
        "    vision = VisionEncoder(model.visual, config.hidden_dim, mean, std)\n",
        "    text = TextEncoder(model, tokenizer, config.hidden_dim)\n",
        "\n",
        "    return vision, text\n",
        "\n",
        "\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, encoder: OpenClipVisionTransformer) -> None:\n",
        "    (...)\n",
        "\n",
        "class _VisionTransformer(nn.Module):\n",
        "    def __init__(self, transformer: OpenClipTransformer) -> None:\n",
        "    (...)\n",
        "\n",
        "class _ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, block: OpenClipResidualAttentionBlock) -> None:\n",
        "    (...)\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model: CLIP, tokenizer: Tokenizer, output_dim: int) -> None:\n",
        "    (...)\n",
        "\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Detector\n",
        "Below two variants have been tested to extract visual entities: OwLViT and GroundingDINO. <br>\n",
        "It is crucial to mention two assumptions on handling borderline cases:\n",
        "- **No boxes detected for an entity**: in this case, for each entity a set of $k$ region of interest as large as the whole image are considered as visual context.\n",
        "- **Overall many objects detected**: detectors can identify many entities given flexible confidence scores (e.g. $0.25$). In instantiating Entity Relationship Pairs, the computational cost grows quadratically and results in memory overflow. To find a tradeoff between tolerance in detection and memory consumption, we have set an upper bound of detected entities of $50$, supporting up to $2500$ pairwise object relationships in the image. Consequently, from the set of detected boxes the top $50$ by confidence are considered."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a. Detector (OwLViT)\n",
        "\n",
        "We test a lightweight, transformer-based, open-set object detector called OwL-ViT ([Minderer et al.](https://arxiv.org/pdf/2205.06230.pdf)) that given a sent of entities, returns multiple object bounding boxes with scores. <br> We apply simple prompt engineering that resulted in improving the detection capabilities:\n",
        "- Providing to OwLViT an entity labeled as `\"[noun]\"` to look for leads to detecting at least one entity in $89\\pm3\\%$ of the RefCOCOg samples.\n",
        "-  Expanding the label to `\"a photo of [noun]\"` boosts the portion of detected entities up to $96\\pm2\\%$ of the RefCOCOg samples.\n",
        "\n",
        "```python\n",
        "class OwlViTDetector(nn.Module):\n",
        "    def __init__(self, config: DetectorConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.owlvit is not None\n",
        "        self._dummy = nn.Parameter(torch.empty(0))\n",
        "        self._box_threshold = config.box_threshold\n",
        "        self._max_detections = config.max_detections\n",
        "        self._preprocessor = OwlViTProcessor.from_pretrained(config.owlvit)\n",
        "        self._detector = OwlViTForObjectDetection.from_pretrained(config.owlvit)\n",
        "        self._freeze()\n",
        "\n",
        "    def _freeze(self) -> None:\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, batch: Batch) -> list[DetectionResults]:\n",
        "        # partialy taken from: https://huggingface.co/docs/transformers/model_doc/owlvit\n",
        "        # Preprocessing\n",
        "        images = [T.to_pil_image(sample.image) for sample in batch.samples]\n",
        "        captions = [sample.caption for sample in batch.samples]\n",
        "\n",
        "        B = len(captions)\n",
        "\n",
        "        # Extracting graph entities\n",
        "        entities: list[list[str]] = [None] * B  # type: ignore\n",
        "        for i, caption in enumerate(captions):\n",
        "            graph = caption.graph\n",
        "            assert graph is not None\n",
        "            entities[i] = [\n",
        "                f\"a photo of {entity.head.lower().strip()}\" for entity in graph.entities\n",
        "            ]\n",
        "\n",
        "        # Object detection (open-vocabulary)\n",
        "        inputs = self._preprocessor(\n",
        "            text=entities, images=images, return_tensors=\"pt\"\n",
        "        ).to(self._dummy.device)\n",
        "        detector_results = self._detector(**inputs)\n",
        "\n",
        "        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
        "        target_sizes = torch.tensor(\n",
        "            [image.size for image in images], device=self._dummy.device\n",
        "        )\n",
        "        # Convert outputs (bounding boxes and class logits) to COCO API\n",
        "        results = self._preprocessor.post_process_object_detection(\n",
        "            outputs=detector_results, target_sizes=target_sizes\n",
        "        )\n",
        "\n",
        "        # For each result\n",
        "        detections: list[DetectionResults] = [None] * B  # type: ignore\n",
        "\n",
        "        for sample_idx in range(B):\n",
        "            boxes, scores, labels = (\n",
        "                results[sample_idx][\"boxes\"],\n",
        "                results[sample_idx][\"scores\"],\n",
        "                results[sample_idx][\"labels\"],\n",
        "            )\n",
        "\n",
        "            matched_indices = []\n",
        "            matched_boxes = []\n",
        "            height, width = images[sample_idx].size\n",
        "\n",
        "            entities_found = [False] * len(entities[sample_idx])\n",
        "\n",
        "            idx = scores >= self._box_threshold\n",
        "            boxes = boxes[idx]\n",
        "            scores = scores[idx]\n",
        "            labels = labels[idx]\n",
        "\n",
        "            # If detections are too many => select tok K first\n",
        "            if len(boxes) > self._max_detections:\n",
        "                _, idx = torch.topk(scores, self._max_detections)\n",
        "                boxes = boxes[idx]\n",
        "                scores = scores[idx]\n",
        "                labels = labels[idx]\n",
        "\n",
        "            # Check identified identities by score\n",
        "            for box, label in zip(boxes, labels):\n",
        "                matched_boxes.append(box)\n",
        "                matched_indices.append(label)\n",
        "                entities_found[label] = True\n",
        "\n",
        "            # If no boxes are found for an entity: suppose the whole image as ROI\n",
        "            for entity_idx, found in enumerate(entities_found):\n",
        "                if not found:\n",
        "                    matched_indices.append(entity_idx)\n",
        "                    matched_boxes.append(\n",
        "                        torch.tensor([0.0, 0.0, width - 1, height - 1]).to(\n",
        "                            self._dummy.device\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            boxes = BBoxes(\n",
        "                boxes=torch.stack(matched_boxes),\n",
        "                images_size=images[sample_idx].size,\n",
        "                format=BBoxFormat.XYXY,\n",
        "                normalized=False,\n",
        "            )\n",
        "\n",
        "            detections[sample_idx] = DetectionResults(\n",
        "                entities=torch.tensor(matched_indices, device=self._dummy.device),\n",
        "                boxes=boxes,\n",
        "            )\n",
        "        return detections\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Detector (GroundingDINO)\n",
        "Alternatively, we test a detector based on GroundingDINO ([Liu et al.](https://arxiv.org/pdf/2303.05499.pdf)) that given an image and a caption, it returns a set of bounding boxes with associated labels from the caption.\n",
        "<br><br>\n",
        "```python\n",
        "class GroundigDINODetector(nn.Module):\n",
        "    def __init__(self, config: DetectorConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.gdino is not None\n",
        "        self._dummy = nn.Parameter(torch.empty(0))\n",
        "        self._mean = (0.485, 0.456, 0.406)\n",
        "        self._std = (0.229, 0.224, 0.225)\n",
        "        self._gdino: GroundingDINO = load_model(\n",
        "            str(config.gdino.config), str(config.gdino.weights)\n",
        "        )\n",
        "        self._box_threshold = config.box_threshold\n",
        "        self._text_threshold = config.text_threshold\n",
        "        self._max_detections = config.max_detections\n",
        "        self._freeze()\n",
        "\n",
        "    def _freeze(self) -> None:\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(\n",
        "        self, images: Nested4DTensor, captions: list[Caption]\n",
        "    ) -> list[DetectionResults]:\n",
        "        \n",
        "        # Image preprocessing\n",
        "        img_tensor = F.normalize(images.tensor, self._mean, self._std)\n",
        "        img_tensor.masked_fill_(images.mask.unsqueeze(1).expand(-1, 3, -1, -1), 0.0)\n",
        "        images = Nested4DTensor(img_tensor, images.sizes, images.mask)\n",
        "        B = len(captions)\n",
        "\n",
        "        entities: list[dict[str, list[int]]] = [{} for _ in range(B)]\n",
        "        sentences: list[str] = [\"\"] * B\n",
        "\n",
        "        # Preprocess the extracted entities as sequences \"noun . noun . noun\" as in the paper\n",
        "        for i, caption in enumerate(captions):\n",
        "            graph = caption.graph\n",
        "            assert graph is not None\n",
        "\n",
        "            for entity_idx, entity in enumerate(graph.entities):\n",
        "                head = entity.head.lower().strip()\n",
        "                entities[i].setdefault(head, []).append(entity_idx)\n",
        "\n",
        "            sentences[i] = \" . \".join(entities[i].keys()) + \" .\"\n",
        "        del i\n",
        "\n",
        "        # Associate category names sequences with image portions\n",
        "        gdino_images = NestedTensor(images.tensor, images.mask)\n",
        "        output = self._gdino(gdino_images, captions=sentences)\n",
        "\n",
        "        pred_logits = output[\"pred_logits\"].sigmoid()\n",
        "        pred_boxes = output[\"pred_boxes\"]\n",
        "\n",
        "        # Apply a defined confidence threshold\n",
        "        masks = pred_logits.max(dim=2)[0] > self._box_threshold\n",
        "\n",
        "        detections: list[DetectionResults] = [None] * B  # type: ignore\n",
        "\n",
        "        for sample_idx in range(B):\n",
        "            mask = masks[sample_idx]\n",
        "            detected_boxes = pred_boxes[sample_idx, mask]\n",
        "            logits = pred_logits[sample_idx, mask] # gDINO logits are phrases\n",
        "\n",
        "            # Extracting the topk most confident boxes\n",
        "            # (avoid combinatorial explosion)\n",
        "            if len(logits) > self._max_detections:\n",
        "                logits, indices = torch.topk(logits, self._max_detections)\n",
        "                detected_boxes = detected_boxes[indices]\n",
        "\n",
        "            tokenized = self._gdino.tokenizer(sentences[sample_idx])\n",
        "\n",
        "            sep_idx = [\n",
        "                i\n",
        "                for i in range(len(tokenized[\"input_ids\"]))\n",
        "                if tokenized[\"input_ids\"][i] in [101, 102, 1012]\n",
        "            ]\n",
        "\n",
        "            phrases: list[str] = []\n",
        "\n",
        "            # processing the gDINO phrases associated with bounding boxes\n",
        "            for logit in logits:\n",
        "                max_idx = logit.argmax()\n",
        "                insert_idx = bisect.bisect_left(sep_idx, max_idx)\n",
        "                right_idx = sep_idx[insert_idx]\n",
        "                left_idx = sep_idx[insert_idx - 1]\n",
        "                phrases.append(\n",
        "                    self._get_phrases_from_posmap(\n",
        "                        logit > self._text_threshold,\n",
        "                        tokenized,\n",
        "                        self._gdino.tokenizer,\n",
        "                        left_idx,\n",
        "                        right_idx,\n",
        "                    ).replace(\".\", \"\")\n",
        "                )\n",
        "\n",
        "            indexes = []\n",
        "            boxes = []\n",
        "            entities_found: list[bool] = [False] * len(\n",
        "                captions[sample_idx].graph.entities  # type: ignore\n",
        "            )\n",
        "\n",
        "            # Phrases are matched with the entities from the SceneGraph\n",
        "            # each entity in the scenegraph is then marked as found\n",
        "            for det_idx, det_name in enumerate(phrases):\n",
        "                if det_name in entities[sample_idx]:\n",
        "                    for entity_idx in entities[sample_idx][det_name]:\n",
        "                        indexes.append(entity_idx)\n",
        "                        boxes.append(detected_boxes[det_idx])\n",
        "                        entities_found[entity_idx] = True\n",
        "                else:\n",
        "                    for entity_name, entity_idxs in entities[sample_idx].items():\n",
        "                        if det_name in entity_name:\n",
        "                            for entity_idx in entity_idxs:\n",
        "                                indexes.append(entity_idx)\n",
        "                                boxes.append(detected_boxes[det_idx])\n",
        "                                entities_found[entity_idx] = True\n",
        "\n",
        "            # Alternatively, the entire image is supposed to be a region of interest (generalization)\n",
        "            for entity_idx, found in enumerate(entities_found):\n",
        "                if not found:\n",
        "                    indexes.append(entity_idx)\n",
        "                    height, width = images.sizes[sample_idx]\n",
        "                    box = torch.tensor(\n",
        "                        [0.0, 0.0, width - 1, height - 1], device=self._dummy.device\n",
        "                    )\n",
        "                    box = box.unsqueeze(0)\n",
        "                    box = ops.from_xyxy_to_cxcywh(box)\n",
        "                    box = ops.normalize(\n",
        "                        box, torch.tensor([[width, height]], device=box.device)\n",
        "                    )\n",
        "                    box = box.squeeze(0)\n",
        "                    boxes.append(box)\n",
        "\n",
        "            detections[sample_idx] = DetectionResults(\n",
        "                entities=torch.tensor(indexes, device=self._dummy.device),\n",
        "                boxes=BBoxes(\n",
        "                    boxes=torch.stack(boxes),\n",
        "                    images_size=images.sizes[sample_idx],\n",
        "                    format=BBoxFormat.CXCYWH,\n",
        "                    normalized=True,\n",
        "                ),\n",
        "            )\n",
        "        return detections\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. ERP Decoder\n",
        "\n",
        "This module consists in a transformer decoder:\n",
        "- For each `(entity1, relationship, entity2)` tuple in a sample's  SceneGraph, the sample image is masked with the union of the gaussian heatmaps centered on the extracted entities `(entity1, entity2)`. This allows the visual encoder to process the original image, while focusing on the target visual entities.\n",
        "- For each `(entity1, relationship, entity2)` tuple, the masked image is encoded with CLIP's visual backbone `ViT-B/32`, which we found to best capture long-range spatial relantionship. We consider the output sequence of path embeddings of the Vision Transformer, while the final CLIP projection is discarded (also for the language backbone).\n",
        "- The visual embeddings sequence of each masked image attends the CLIP token embeddings of the respective `(relationship)` with a stack of cross-attention layers. Recalling the SceneGraph parsing paragraph: a `(relationship` consists in a substring of the original image caption, extracted with a semantic parser. \n",
        "<br><br>\n",
        "The procedure described above is implemented with the aid of a `Graph` data structure:\n",
        "- A `Graph` represents the relationships between entities in each scene, that is a sample, described from the image and the caption. \n",
        "- The `nodes` consist in all the entities found by the Detector in the image.\n",
        "- The `edges` are derived from the SceneGraph parsing of the caption. Only the edges between detected entities are considered.\n",
        "<br><br>\n",
        "```python\n",
        "@dataclass(frozen=True)\n",
        "class Graph:\n",
        "    nodes: Float[Tensor, \"N D\"]\n",
        "    edges: Float[Tensor, \"M D\"]\n",
        "    edge_index: Int64[Tensor, \"2 M\"]\n",
        "\n",
        "    @classmethod\n",
        "    def new(\n",
        "        cls,\n",
        "        caption: Caption,\n",
        "        embeddings: CaptionEmbeddings,\n",
        "        detections: DetectionResults,\n",
        "    ) -> Self:\n",
        "        \n",
        "        # A caption is a scenegraph\n",
        "        graph = caption.graph\n",
        "        assert graph is not None\n",
        "\n",
        "        # The detected entities are nodes\n",
        "        nodes = embeddings.entities[detections.entities]\n",
        "\n",
        "        edge_index_list = []\n",
        "        edge_rel_index_list = []\n",
        "\n",
        "        for idx, detection in enumerate(detections.entities):\n",
        "            entity: int = detection.item()\n",
        "\n",
        "            # Build edges from the scenegraph's connections\n",
        "            for connection in graph.connections(entity):\n",
        "                if connection.end < entity:\n",
        "                    continue\n",
        "\n",
        "                device = detections.entities[0].device\n",
        "                tmp = (\n",
        "                    (detections.entities == connection.end)\n",
        "                    .nonzero(as_tuple=True)[0]\n",
        "                    .to(device)\n",
        "                )\n",
        "                indexes = torch.cat(\n",
        "                    [\n",
        "                        torch.tensor([idx])[None].expand(1, len(tmp)).to(device),\n",
        "                        tmp[None],\n",
        "                    ],\n",
        "                    dim=0,\n",
        "                )\n",
        "\n",
        "                edge_index_list.append(indexes)\n",
        "                edge_rel_index_list.extend([connection.relation] * len(tmp))\n",
        "\n",
        "        # No relationships found in the caption\n",
        "        if len(edge_index_list) == 0:\n",
        "            edge_index = torch.empty(\n",
        "                (2, 0),\n",
        "                dtype=torch.long,\n",
        "                device=detections.entities.device,\n",
        "            )\n",
        "        else:\n",
        "            edge_index = torch.cat(edge_index_list, dim=1)  # (2, M)\n",
        "\n",
        "        edge_rel_index = torch.tensor(\n",
        "            edge_rel_index_list,\n",
        "            dtype=torch.long,\n",
        "            device=embeddings.relations.device,\n",
        "        )  # (M,)\n",
        "        edges = embeddings.relations[edge_rel_index]\n",
        "\n",
        "        return cls(nodes, edges, edge_index)\n",
        "```\n",
        "\n",
        "<br><br>\n",
        "A batch of `Graph` objects is organized in a wrapper data structure, namely a `NestedGraph`.\n",
        "<br><br>\n",
        "```python\n",
        "class NestedGraph:\n",
        "    def __init__(\n",
        "        self,\n",
        "        nodes: Float[Tensor, \"B N D\"],\n",
        "        edges: Float[Tensor, \"B E D\"],\n",
        "        edge_index: Int64[Tensor, \"2 BE\"],\n",
        "        sizes: list[tuple[int, int]],\n",
        "    ) -> None:\n",
        "        self._nodes = nodes\n",
        "        self._edges = edges\n",
        "        self._edge_index = edge_index\n",
        "        self._sizes = sizes\n",
        "    (...)\n",
        "```\n",
        "<br><br>\n",
        "\n",
        "In order to process a batch of Graph objects, padding is applied in the number of nodes and edges (in order to concatenate the nodes,edges embeddings):\n",
        "<br><br>\n",
        "```python\n",
        "def pad_sequences(\n",
        "    detections: list[DetectionResults],\n",
        "    graphs: list[Graph],\n",
        ") -> tuple[BBoxes, NestedGraph]:\n",
        "    if len(detections) != len(graphs):\n",
        "        raise ValueError(\n",
        "            f\"The number of detections ({len(detections)}) must be equal \"\n",
        "            f\"to the number of graphs ({len(graphs)})\"\n",
        "        )\n",
        "\n",
        "    batch = len(detections)\n",
        "    sizes = [(graph.nodes.shape[0], graph.edges.shape[0]) for graph in graphs]\n",
        "    max_nodes = max([nodes for nodes, _ in sizes]) + 1\n",
        "    max_edges = max([edges for _, edges in sizes])\n",
        "\n",
        "    # We add one to the number of max_nodes to prevent the situation in which\n",
        "    # the graph with max_nodes is not the graph with max_edges.\n",
        "    # If we did not do this, when padding the edge_index we would add an edge\n",
        "    # between two non existent padding nodes (graph.nodes.shape[0] == max_nodes)\n",
        "    # causing an index out of bounds error.\n",
        "\n",
        "    padded_boxes = detections[0].boxes.tensor.new_ones(batch * max_nodes, 4)\n",
        "    images_size = detections[0].boxes.images_size.new_ones(batch * max_nodes, 2)\n",
        "\n",
        "    nodes = graphs[0].nodes.new_zeros(batch, max_nodes, graphs[0].nodes.shape[1])\n",
        "    edges = graphs[0].edges.new_zeros(batch, max_edges, graphs[0].edges.shape[1])\n",
        "    edge_index = graphs[0].edge_index.new_empty(2, batch * max_edges)\n",
        "\n",
        "    for i, (detection, graph) in enumerate(zip(detections, graphs)):\n",
        "        nodes[i, : graph.nodes.shape[0]].copy_(graph.nodes)\n",
        "        edges[i, : graph.edges.shape[0]].copy_(graph.edges)\n",
        "\n",
        "        # Pad edge index\n",
        "        start = i * max_edges\n",
        "        middle = start + graph.edge_index.shape[1]\n",
        "        end = start + max_edges\n",
        "\n",
        "        edge_index[:, start:middle].copy_(graph.edge_index + i * max_nodes)\n",
        "        edge_index[0, middle:end] = graph.nodes.shape[0] + i * max_nodes\n",
        "        edge_index[1, middle:end] = graph.nodes.shape[0] + i * max_nodes\n",
        "\n",
        "        # Pad boxes\n",
        "        start = i * max_nodes\n",
        "        boxes = detection.boxes.to_cxcywh().normalize()\n",
        "        end = start + len(boxes)\n",
        "\n",
        "        padded_boxes[start:end].copy_(boxes.tensor)\n",
        "        images_size[start:end].copy_(boxes.images_size)\n",
        "\n",
        "    boxes = BBoxes(padded_boxes, images_size, BBoxFormat.CXCYWH, True)\n",
        "\n",
        "    return boxes, NestedGraph(nodes, edges, edge_index, sizes)\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the Decoder module is implemented as follows:\n",
        "<br><br>\n",
        "```python\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: DecoderConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._num_heads = config.num_heads\n",
        "        self._hidden_dim = config.hidden_dim\n",
        "        self._same_entity_edge = nn.Parameter(torch.randn(1, config.hidden_dim))\n",
        "\n",
        "        # self._patch_encondings = PatchSpatialEncodings(config.hidden_dim)\n",
        "        self._gaussian_heatmaps = GaussianHeatmaps()\n",
        "\n",
        "        self._layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: Nested4DTensor,\n",
        "        graph: NestedGraph,\n",
        "        boxes: BBoxes,\n",
        "    ) -> NestedGraph:\n",
        "        H, W = images.shape[2:]\n",
        "\n",
        "        # (entity1, entity2), edges\n",
        "        edge_index = graph.edge_index(False)  # (2 BE)\n",
        "\n",
        "        # Select bboxes that have a connection (entity pairs)\n",
        "        boxes1 = boxes[edge_index[0]]  # (BE 4)\n",
        "        boxes2 = boxes[edge_index[1]]  # (BE 4)\n",
        "\n",
        "        # Compute the union of the boxes and the respective gaussian heatmaps\n",
        "        union_boxes = boxes1.union(boxes2)  # (BE 4)\n",
        "\n",
        "        heatmaps = self._gaussian_heatmaps(boxes, (H, W))  # (BN HW)\n",
        "        union_heatmaps = self._gaussian_heatmaps(union_boxes, (H, W))  # (BE, HW)\n",
        "        heatmaps1 = heatmaps[edge_index[0]]  # (BE HW)\n",
        "        heatmaps2 = heatmaps[edge_index[1]]  # (BE HW)\n",
        "\n",
        "        edge_heatmaps = torch.maximum(\n",
        "            torch.maximum(heatmaps1, heatmaps2),\n",
        "            union_heatmaps,\n",
        "        )  # (BE HW)\n",
        "\n",
        "        heatmaps = torch.log(heatmaps + 1e-8)  # (BN HW)\n",
        "        edge_heatmaps = torch.log(edge_heatmaps + 1e-8)  # (BE HW)\n",
        "\n",
        "        node_heatmaps = heatmaps.view(len(graph), -1, H * W)  # (B N HW)\n",
        "        edge_heatmaps = edge_heatmaps.view(len(graph), -1, H * W)  # (B E HW)\n",
        "        heatmaps = torch.cat((node_heatmaps, edge_heatmaps), dim=1)  # (B (N+E) HW)\n",
        "\n",
        "        flattened_images = images.flatten()  # (B HW D)\n",
        "        masks = flattened_images.mask.unsqueeze(1).expand(-1, heatmaps.shape[1], -1)\n",
        "        masks = heatmaps.masked_fill_(masks, -torch.inf)  # (B (N+E) HW)\n",
        "        masks = masks.repeat(self._num_heads, 1, 1)  # (Bh (N+E) HW)\n",
        "\n",
        "        nodes = graph.nodes(True)  # (B N D)\n",
        "        edges = graph.edges(True)  # (B E D)\n",
        "        x = torch.cat((nodes, edges), dim=1)  # (B (N+E) D)\n",
        "\n",
        "        # One decoded layer \n",
        "        for block in self._layers:\n",
        "            x = block(\n",
        "                x,\n",
        "                flattened_images.tensor,\n",
        "                masks,\n",
        "            )\n",
        "\n",
        "        nodes = x[:, : nodes.shape[1]]  # (B N D)\n",
        "        edges = x[:, nodes.shape[1] :]  # (B E D)\n",
        "\n",
        "        return graph.new_like(nodes, edges)\n",
        "\n",
        "    def __call__(\n",
        "        self, images: Nested4DTensor, graph: NestedGraph, boxes: BBoxes\n",
        "    ) -> NestedGraph:\n",
        "        return super().__call__(images, graph, boxes)  # type: ignore\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: DecoderConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # attention with images\n",
        "        self._norm1 = nn.LayerNorm(config.hidden_dim)\n",
        "        self._attn = nn.MultiheadAttention(\n",
        "            embed_dim=config.hidden_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self._layerscale1 = LayerScale(config.hidden_dim, config.epsilon_layer_scale)\n",
        "\n",
        "        # feedforward\n",
        "        self._norm2 = nn.LayerNorm(config.hidden_dim)\n",
        "        self._ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, 4 * config.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(4 * config.hidden_dim, config.hidden_dim),\n",
        "        )\n",
        "        self._layerscale2 = LayerScale(config.hidden_dim, config.epsilon_layer_scale)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Float[Tensor, \"B (N+E) D\"],\n",
        "        images: Float[Tensor, \"B HW D\"],\n",
        "        mask: Float[Tensor, \"Bh (N+E) HW\"],\n",
        "    ) -> Float[Tensor, \"B (N+E) D\"]:\n",
        "        # image attention\n",
        "        x1 = self._norm1(x)  # (B (N+E) D)\n",
        "        x1, _ = self._attn(\n",
        "            x1,\n",
        "            images,\n",
        "            images,\n",
        "            attn_mask=mask,\n",
        "            need_weights=False,\n",
        "        )  # (B (N+E) D)\n",
        "        x1 = x + self._layerscale1(x1)  # (B (N+E) D)\n",
        "\n",
        "        # feedforward\n",
        "        x2 = self._norm2(x1)  # (B (N+E) D)\n",
        "        x2 = self._ffn(x2)  # (B (N+E) D)\n",
        "        x2 = x1 + self._layerscale2(x2)  # (B (N+E) D)\n",
        "\n",
        "        return x2  # type: ignore\n",
        "\n",
        "\n",
        "# taken from https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/transformer.py\n",
        "class LayerScale(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        init_value: float = 0.1,\n",
        "        inplace: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._inplace = inplace\n",
        "        self._scale = nn.Parameter(torch.ones(dim) * init_value)\n",
        "\n",
        "    def forward(self, x: Float[Tensor, \"B ...\"]) -> Float[Tensor, \"B ...\"]:\n",
        "        return x.mul_(self._scale) if self._inplace else x * self._scale\n",
        "```\n",
        "\n",
        "<br><br>\n",
        "Where each `DecoderLayer` in the `Decoder` consists in a vanilla Transformer's decoder block. For each sample: \n",
        "- As `Keys, Values`, the CLIP-encoded masked image patches are considered. \n",
        "- As `Query`, the concatenated nodes and edges of the sample scene graph are considered. Nodes and edges consist respectively in the CLIP encoded token sequences for the detected entity names and the parsed relationships."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Regression Head\n",
        "\n",
        "The last module is a Transformer's Encoder:\n",
        "- For each sample graph, positional (cosine) encoding is applied to the embeddings (nodes+edges).\n",
        "- The language-informed (graph) visual tokens attend to each other in each scene. A learnable token is added to the input sequence to capture the scene semantics.\n",
        "- The regression token is eventually projected to bounding box logits to predict the final ROI. \n",
        "- Self-attention between graph embeddings is computed batch-wise.\n",
        "<br><br>\n",
        "```python\n",
        "class RegressionHead(nn.Module):\n",
        "    def __init__(self, config: HeadConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._dim = config.hidden_dim\n",
        "        self._token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n",
        "\n",
        "        self._layers = nn.ModuleList(\n",
        "            [ResidualAttentionLayer(config) for _ in range(config.num_layers)]\n",
        "        )\n",
        "\n",
        "        self._regression_head = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.hidden_dim, 4),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _positional_encoding(self, mask: Bool[Tensor, \"B L\"]) -> Float[Tensor, \"B L D\"]:\n",
        "        \"\"\" Sentence-wise positional embedding (cosine) \"\"\"\n",
        "        not_mask = ~mask  # (B, L)\n",
        "        embed = not_mask.cumsum(dim=1, dtype=torch.float32)  # (B, L)\n",
        "\n",
        "        dim_t = torch.arange(self._dim, dtype=torch.float32, device=mask.device)\n",
        "        dim_t = 10000 ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self._dim)\n",
        "\n",
        "        pos = embed[:, :, None] / dim_t  # (B, L, D)\n",
        "        pos = torch.stack((pos[..., 0::2].sin(), pos[..., 1::2].cos()), dim=3)\n",
        "        pos = pos.view(pos.shape[0], pos.shape[1], -1)  # (B, L, D)\n",
        "\n",
        "        return pos\n",
        "\n",
        "    def forward(self, graph: NestedGraph, images_size: list[tuple[int, int]]) -> BBoxes:\n",
        "\n",
        "        B = len(graph)\n",
        "        nodes = graph.nodes(True)  # (B, N, D)\n",
        "        edges = graph.edges(True)  # (B, E, D)\n",
        "        # Each graph is a concatenation of nodes and edges\n",
        "        tokens = torch.cat([nodes, edges], dim=1)  # (B, N+E, D)\n",
        "        N, E = nodes.shape[1], edges.shape[1]\n",
        "\n",
        "        # Pad masking\n",
        "        mask = nodes.new_ones((B, 1 + N + E), dtype=torch.bool)  # (B, 1+N+E)\n",
        "        for idx, (num_nodes, num_edges) in enumerate(graph.sizes):\n",
        "            mask[idx, 0] = False\n",
        "            mask[idx, 1 : 1 + num_nodes] = False\n",
        "            mask[idx, 1 + N : 1 + N + num_edges] = False\n",
        "\n",
        "        # Apply positional encoding (entity-wise)\n",
        "        tokens = tokens + self._positional_encoding(mask[:, 1:])  # (B, N+E, D)\n",
        "\n",
        "        # Concatenate all node-edges\n",
        "        # concatenate also the learnable [REG] token\n",
        "        x = torch.cat([self._token.expand(B, -1, -1), tokens], dim=1)  # (B, 1+N+E, D)\n",
        "\n",
        "        # Masked self attention\n",
        "        for layer in self._layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Regression on the learnable token\n",
        "        token = self._regression_head(x[:, 0])  # (B, 4)\n",
        "        boxes = BBoxes(token, images_size, BBoxFormat.CXCYWH, True)  # (B, 4)\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __call__(\n",
        "        self, graph: NestedGraph, images_size: list[tuple[int, int]]\n",
        "    ) -> BBoxes:\n",
        "        return super().__call__(graph, images_size)  # type: ignore\n",
        "\n",
        "\n",
        "class ResidualAttentionLayer(nn.Module):\n",
        "    def __init__(self, config: HeadConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._norm1 = nn.LayerNorm(config.hidden_dim)\n",
        "        self._attn = nn.MultiheadAttention(\n",
        "            embed_dim=config.hidden_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self._dropout1 = nn.Dropout(config.dropout)\n",
        "\n",
        "        self._norm2 = nn.LayerNorm(config.hidden_dim)\n",
        "        self._ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, 4 * config.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(4 * config.hidden_dim, config.hidden_dim),\n",
        "        )\n",
        "        self._dropout2 = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"B L D\"], mask: Bool[Tensor, \"B L\"]\n",
        "    ) -> Float[Tensor, \"B L D\"]:\n",
        "        x1 = self._norm1(x)\n",
        "        x1 = self._attn(x1, x1, x1, key_padding_mask=mask, need_weights=False)[0]\n",
        "        x1 = self._dropout1(x1)\n",
        "        x1 = x + x1\n",
        "\n",
        "        x2 = self._norm2(x1)\n",
        "        x2 = self._ffn(x2)\n",
        "        x2 = self._dropout2(x2)\n",
        "        x2 = x1 + x2\n",
        "\n",
        "        return x2  # type: ignore\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture overview\n",
        "\n",
        "The described modules are combined according to the following pipeline:\n",
        "- A batch of `(image,caption)` is firstly processed with CLIP to extract the sequences of embeddings.\n",
        "- The processed batch is fed to the Detector to extract the `DetectionResults`.\n",
        "- The resulting batch of graphs, with the encoded entity pairs, is fed to the `Decoder` to fuse the modalities.\n",
        "- Eventually, the batch of graphs is fed to the `RegressionHead` such that all entity pairs within each graph attend to each other.\n",
        "<br><br>\n",
        "The final model is implemented in `src/visgator/models/erpa/_model.py`:\n",
        "\n",
        "```python\n",
        "class Model(_Model[BBoxes]):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._transform = Compose(\n",
        "            [Resize(800, max_size=1333, p=1.0)],\n",
        "            p=1.0,\n",
        "        )\n",
        "\n",
        "        self._criterion = Criterion(config.criterion)\n",
        "        self._postprocessor = PostProcessor()\n",
        "\n",
        "        # Encoders\n",
        "        self._vision, self._text = build_encoders(config.encoders)\n",
        "        self._decoder = Decoder(config.decoder)\n",
        "\n",
        "        # Detector\n",
        "        self._gdino = None\n",
        "        self._owlvit = None\n",
        "        if config.detector.gdino is not None:\n",
        "            self._gdino = GroundigDINODetector(config.detector)\n",
        "        elif config.detector.owlvit is not None:\n",
        "            self._owlvit = OwlViTDetector(config.detector)\n",
        "\n",
        "        # Regression Head\n",
        "        self._head = RegressionHead(config.head)\n",
        "    (...)\n",
        "\n",
        "    def forward(self, batch: Batch) -> BBoxes:\n",
        "\n",
        "        # Preprocess the images for the detector\n",
        "        images = Nested4DTensor.from_tensors(\n",
        "            [self._transform(sample.image) for sample in batch.samples]\n",
        "        )\n",
        "        img_tensor = images.tensor / 255.0\n",
        "        images = Nested4DTensor(img_tensor, images.sizes, images.mask)\n",
        "\n",
        "        # Detection results extraction\n",
        "        if self._gdino is not None:\n",
        "            detections = self._gdino(\n",
        "                images, [sample.caption for sample in batch.samples]\n",
        "            )\n",
        "        elif self._owlvit is not None:\n",
        "            detections = self._owlvit(batch)\n",
        "        else:\n",
        "            raise RuntimeError(\"No detector is initialized.\")\n",
        "\n",
        "        # CLIP encoded img+text\n",
        "        img_embeddings = self._vision(images)\n",
        "        text_embeddings = self._text(batch)\n",
        "\n",
        "        # Constructing the batch graphs with entity embeddings\n",
        "        graphs = [\n",
        "            Graph.new(batch.samples[idx].caption, text_embeddings[idx], detections[idx])\n",
        "            for idx in range(len(batch))\n",
        "        ]\n",
        "\n",
        "        boxes, graph = pad_sequences(detections, graphs)\n",
        "        \n",
        "        # ERP-Cross Attention (img+text)\n",
        "        graph = self._decoder(img_embeddings, graph, boxes)\n",
        "\n",
        "        # ERP-Self Attention and bbox regression\n",
        "        boxes = self._head(graph, img_embeddings.sizes)\n",
        "        \n",
        "        if not boxes.normalized:\n",
        "            raise RuntimeError(\"Boxes must be normalized.\")\n",
        "\n",
        "        boxes = BBoxes(\n",
        "            boxes.tensor,\n",
        "            [tuple(sample.image.shape[1:]) for sample in batch],  # type: ignore\n",
        "            boxes.format,\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        return boxes\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenges\n",
        "- When the object detector does not find the entities, the whole image is taken as bounding box for each entity.\n",
        "- OwLViT and YOLO are limited, GroundingDINO is expensive.\n",
        "- The dataset is noisy: grammatical errors, phrases with incomplete syntax. This leads to low quality entities detected from text and thus degraded comprehension of the textual grounding. The model may overfit the training set in order to yield high accuracy, with lack of generalization capabilities.\n",
        "- Memory leaking: it required debug and meticoulus tracking of the computational graph to make sure no residuals are present. Garbage collection is also called after every backpropagation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n",
        "\n",
        "### [below there is random and drafts]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CAKytUi3fTRa"
      },
      "source": [
        "### Preliminaries: downloading data & code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32rW94cRQzPf"
      },
      "outputs": [],
      "source": [
        "# Downloading the dataset\n",
        "# !pip -q install gdown\n",
        "# !gdown 1hxk0f62WtczYGp_zMBE_SQuqfsUvSNld\n",
        "\n",
        "# Extracting the dataset locally\n",
        "# !apt-get install unrar\n",
        "# !unrar x -vb refcocog.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruRmOSPxUD8d",
        "outputId": "ba189617-d5e3-44a9-abc4-94c688acf5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Deep Learning 2023\n"
          ]
        }
      ],
      "source": [
        "# Alternatively, mounting a Drive folder with data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/\"Deep Learning 2023\"\n",
        "# !unrar x -vb refcocog.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7iQ49ZkfwhT"
      },
      "outputs": [],
      "source": [
        "# Repo\n",
        "!rm -rf visgator\n",
        "!git clone https://github.com/FrancescoGentile/visgator\n",
        "%cd visgator\n",
        "!git checkout detector\n",
        "!git pull\n",
        "!pip3 install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-lfv-eQNZkM",
        "outputId": "fbdd0aea-6817-4280-d98c-3019c13a3944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for visgator (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9ZUTkvpmHUJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from ruamel.yaml import YAML\n",
        "\n",
        "# Loading the model configuration file from repo\n",
        "config_path = Path(\"config/local.yaml\")\n",
        "extention = config_path.suffix\n",
        "\n",
        "match extention:\n",
        "    case \".json\":\n",
        "        with open(config_path, \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "    case \".yaml\":\n",
        "        yaml = YAML(typ=\"safe\")\n",
        "        cfg = yaml.load(config_path)\n",
        "    case _:\n",
        "        raise ValueError(f\"Unknown config file extention: {extention}.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ORllucLOXRb8"
      },
      "source": [
        "### 2. Instantiation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EUDHa-8FlamR"
      },
      "source": [
        "As subsequently shown, the training starts with the instatiation of a `visgator.engines.Trainer` object, followed by `Trainer.run()`. Under the hood, for `num_epochs` the method `Trainer._train_epoch(epoch)` is invoked:\n",
        "\n",
        "```python\n",
        "def _train_epoch(self, epoch: int) -> None:\n",
        "        self._logger.info(f\"Training epoch {epoch + 1} started.\")\n",
        "\n",
        "        start = timer()\n",
        "\n",
        "        self._model.train()\n",
        "        self._postprocessor.train()\n",
        "        self._criterion.train()\n",
        "\n",
        "        self._tl_tracker.increment()\n",
        "        self._tm_tracker.increment()\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "        counter = tqdm(\n",
        "            desc=\"Training\",\n",
        "            total=self._get_steps_per_epoch() * self._params.train_batch_size,\n",
        "        )\n",
        "\n",
        "        with counter as progress_bar:\n",
        "            batch: Batch\n",
        "            bboxes: BBoxes\n",
        "            device_type = \"cuda\" if self._device.is_cuda else \"cpu\"\n",
        "            for idx, (batch, bboxes) in enumerate(self._train_loader):\n",
        "                # since the dataloader batch size is equal to true batch size\n",
        "                # // gradient accumulation steps, the last samples in the dataloader\n",
        "                # may not be enough to fill the true batch size, so we break the loop\n",
        "                # for example, if the true batch size is 8, the gradient accumulation\n",
        "                # steps is 4 and the dataset size is 50, the last 2 samples will be\n",
        "                # ignored\n",
        "                if progress_bar.total == progress_bar.n:\n",
        "                    break\n",
        "\n",
        "                batch = batch.to(self._device.to_torch())\n",
        "                bboxes = bboxes.to(self._device.to_torch())\n",
        "\n",
        "                with autocast(device_type, enabled=self._params.mixed_precision):\n",
        "                    outputs = self._model(batch)\n",
        "                    tmp_losses = self._criterion(outputs, bboxes)\n",
        "                    losses = self._tl_tracker(tmp_losses)\n",
        "                    loss = losses.total / self._params.gradient_accumulation_steps\n",
        "\n",
        "                self._scaler.scale(loss).backward()\n",
        "\n",
        "                if (idx + 1) % self._params.gradient_accumulation_steps == 0:\n",
        "                    if self._params.max_grad_norm is not None:\n",
        "                        self._scaler.unscale_(self._optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(  # type: ignore\n",
        "                            self._model.parameters(), self._params.max_grad_norm\n",
        "                        )\n",
        "\n",
        "                    self._scaler.step(self._optimizer)\n",
        "                    self._scaler.update()\n",
        "                    self._optimizer.zero_grad()\n",
        "                    self._lr_scheduler.step_after_batch()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    pred_bboxes = self._postprocessor(outputs)\n",
        "                    self._tm_tracker.update(\n",
        "                        pred_bboxes.to_xyxy().normalize().tensor,\n",
        "                        bboxes.to_xyxy().normalize().tensor,\n",
        "                    )\n",
        "\n",
        "                progress_bar.update(len(batch))\n",
        "                \n",
        "                del batch\n",
        "                del bboxes\n",
        "                del outputs\n",
        "                del loss\n",
        "\n",
        "                # Empty cache at each sample\n",
        "                if self._device.is_cuda:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        self._lr_scheduler.step_after_epoch()\n",
        "\n",
        "        end = timer()\n",
        "        elapsed = end - start\n",
        "\n",
        "        self._logger.info(f\"Training epoch {epoch + 1} finished.\")\n",
        "        self._log_statistics(epoch, elapsed, train=True)\n",
        "```\n",
        "\n",
        "The ERP-Attention model is invoked in `visgator.engines.trainer.Trainer.train_epoch()` and it is implemented in `visgator.models.erpa.Model.forward()` as follows:\n",
        "\n",
        "```python\n",
        "def forward(self, batch: Batch) -> BBoxes:\n",
        "        images = Nested4DTensor.from_tensors(\n",
        "            [self._transform(sample.image) for sample in batch.samples]\n",
        "        )\n",
        "        img_tensor = images.tensor / 255.0\n",
        "        images = Nested4DTensor(img_tensor, images.sizes, images.mask)\n",
        "        detections = self._detector((batch, images), (self._model, self._tokenizer))\n",
        "\n",
        "        # CLIP encoded img+text\n",
        "        img_embeddings = self._vision(images)\n",
        "        text_embeddings = self._text(batch)\n",
        "\n",
        "        # Constructing the batch graphs with entity embeddings\n",
        "        graphs = [\n",
        "            Graph.new(batch.samples[idx].caption, text_embeddings[idx], detections[idx])\n",
        "            for idx in range(len(batch))\n",
        "        ]\n",
        "\n",
        "        boxes, graph = pad_sequences(detections, graphs)\n",
        "        graph = self._decoder(img_embeddings, graph, boxes)\n",
        "\n",
        "        # ERP-Cross Attention (img+text)\n",
        "        boxes = self._head(graph, img_embeddings.sizes)\n",
        "        if not boxes.normalized:\n",
        "            raise RuntimeError(\"Boxes must be normalized.\")\n",
        "\n",
        "        boxes = BBoxes(\n",
        "            boxes.tensor,\n",
        "            [tuple(sample.image.shape[1:]) for sample in batch],  # type: ignore\n",
        "            boxes.format,\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        return boxes\n",
        "```\n",
        "\n",
        "Each `(image, caption)` pair is encoded with respectively the vision and the text backbone of CLIP (ViT-B/32). Note that the final projections leading to the common embedding space are discarded (`visgator.models.erpa._model.forward()`).\n",
        "\n",
        "In parallel, each `(image, caption)` pair is also processed with a detector(e.g. [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)/[YOLOv8](https://github.com/ultralytics/ultralytics)/[OwL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)), implemented with `visgator.models.erpa._detector.Detector`, to generate region proposals for each entity (`visgator.models.erpa._misc.Graph.new()`).\n",
        "\n",
        "The per-entity region proposals, the text embeddings and the caption are embedded in a Nested SceneGraph, that is a set of graphs for the training batch. Moreover, the padded region proposals are organized in `visgator.utils.bbox.BBoxes` objects to apply handy geometrical operators.\n",
        "Finally, a `NestedGraph` object is passed to the ERP-Decoder, along with the formatted bounding boxes and the image embeddings."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PsYx7aueqVH_"
      },
      "source": [
        "\n",
        "The invocation of `self._detector(...)` is implemented in `visgator.models.erpa.Detector.forward()`:\n",
        "\n",
        "```python\n",
        "def forward(self, data: tuple, model: tuple) -> list[DetectionResults]:\n",
        "        # partialy taken from: https://huggingface.co/docs/transformers/model_doc/owlvit\n",
        "        \n",
        "        batch, nested_images = data\n",
        "        clip, tokenizer = model\n",
        "\n",
        "        # Preprocessing & YOLO\n",
        "        images = [self._toPIL(sample.image) for sample in batch.samples]\n",
        "        captions = [sample.caption for sample in batch.samples]\n",
        "        \n",
        "        B = len(captions)\n",
        "\n",
        "        # Extracting graph entities\n",
        "        entities: list[list[str]] = [None] * B  # type: ignore\n",
        "        for i, caption in enumerate(captions):\n",
        "            graph = caption.graph\n",
        "            assert graph is not None\n",
        "            entities[i] = [entity.head.lower().strip() for entity in graph.entities]\n",
        "\n",
        "        # Object detection (open-vocabulary)\n",
        "        with torch.no_grad():\n",
        "            inputs = self._detector_processor(text=entities, images=images, return_tensors=\"pt\").to(self.device)\n",
        "            detector_results = self._detector_model(**inputs)\n",
        "\n",
        "            # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
        "            target_sizes = torch.Tensor([image.size for image in images]).to(self.device)\n",
        "            # Convert outputs (bounding boxes and class logits) to COCO API\n",
        "            results = self._detector_processor.post_process_object_detection(outputs=detector_results, target_sizes=target_sizes)\n",
        "\n",
        "        # For each result\n",
        "        detections: list[DetectionResults] = [None] * B  # type: ignore\n",
        "\n",
        "        \n",
        "        for sample_idx in range(B):\n",
        "            boxes, scores, labels = results[sample_idx][\"boxes\"], results[sample_idx][\"scores\"], results[sample_idx][\"labels\"]\n",
        "            \n",
        "            matched_indices = []\n",
        "            matched_boxes = []\n",
        "            height, width = images[sample_idx].size\n",
        "\n",
        "            # Check identified identities by score\n",
        "            for j, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
        "                box = [round(i, 2) for i in box.tolist()]\n",
        "                if score >= self._detection_threshold:\n",
        "                    matched_boxes.append(torch.tensor(box))\n",
        "                    matched_indices.append(label)\n",
        "                # not detected => suppose the entire image\n",
        "                else:\n",
        "                    matched_boxes.append(torch.tensor([0, 0, width-1, height-1]).to(self.device))\n",
        "                    matched_indices.append(j) # entity index\n",
        "               \n",
        "            # if the detector hasn't identified an object => whole image as bounding box\n",
        "            if len(boxes) == 0:\n",
        "                for entity_idx, entity in enumerate(entities[sample_idx]):\n",
        "                    matched_indices.append(entity_idx)\n",
        "                    matched_boxes.append(torch.tensor([0, 0, width-1, height-1]).to(self.device))\n",
        "            \n",
        "\n",
        "            boxes = BBoxes(\n",
        "                boxes=torch.stack(matched_boxes).to(self.device),\n",
        "                images_size=images[sample_idx].size,\n",
        "                format=BBoxFormat.XYXY,\n",
        "                normalized=False,\n",
        "            ).to_cxcywh().normalize()\n",
        "\n",
        "            detections[sample_idx] = DetectionResults(\n",
        "                entities=torch.tensor(matched_indices, device=self.device, dtype=torch.int),\n",
        "                boxes=boxes,\n",
        "            )\n",
        "\n",
        "        del inputs\n",
        "        del detector_results\n",
        "        del target_sizes\n",
        "        del results\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return detections\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D85DeiT4nXq4"
      },
      "source": [
        "### 3. Decoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "emfoF9tCnm8c"
      },
      "source": [
        "\n",
        "In each ERP, a gaussian heatmap is computed for each entity bounding box. The union of the heatmaps constitutes the mask for the input image (`visgator.models.erpa.Decoder.forward()`). A stack of attention layers processes the ERPs so that the visual tokens attend the text embeddings, eventually re-arranged in nodes and edges of the NestedGraph.\n",
        "\n",
        "```python\n",
        "def forward(\n",
        "        self,\n",
        "        images: Nested4DTensor,\n",
        "        graph: NestedGraph,\n",
        "        boxes: BBoxes,\n",
        "    ) -> NestedGraph:\n",
        "        H, W = images.shape[2:]\n",
        "\n",
        "        # (entity1, entity2), edges\n",
        "        edge_index = graph.edge_index(False)  # (2 BE)\n",
        "\n",
        "        # Select bboxes that have a connection\n",
        "        boxes1 = boxes[edge_index[0]]  # (BE 4)\n",
        "        boxes2 = boxes[edge_index[1]]  # (BE 4)\n",
        "\n",
        "        union_boxes = boxes1.union(boxes2)  # (BE 4)\n",
        "\n",
        "        heatmaps = self._gaussian_heatmaps(boxes, (H, W))  # (BN HW)\n",
        "        union_heatmaps = self._gaussian_heatmaps(union_boxes, (H, W))  # (BE, HW)\n",
        "        heatmaps1 = heatmaps[edge_index[0]]  # (BE HW)\n",
        "        heatmaps2 = heatmaps[edge_index[1]]  # (BE HW)\n",
        "\n",
        "        edge_heatmaps = torch.maximum(\n",
        "            torch.maximum(heatmaps1, heatmaps2),\n",
        "            union_heatmaps,\n",
        "        )  # (BE HW)\n",
        "\n",
        "        heatmaps = torch.log(heatmaps + 1e-8)  # (BN HW)\n",
        "        edge_heatmaps = torch.log(edge_heatmaps + 1e-8)  # (BE HW)\n",
        "\n",
        "        node_heatmaps = heatmaps.view(len(graph), -1, H * W)  # (B N HW)\n",
        "        edge_heatmaps = edge_heatmaps.view(len(graph), -1, H * W)  # (B E HW)\n",
        "        heatmaps = torch.cat((node_heatmaps, edge_heatmaps), dim=1)  # (B (N+E) HW)\n",
        "\n",
        "        flattened_images = images.flatten()  # (B HW D)\n",
        "        masks = flattened_images.mask.unsqueeze(1).expand(-1, heatmaps.shape[1], -1)\n",
        "        masks = heatmaps.masked_fill_(masks, -torch.inf)  # (B (N+E) HW)\n",
        "        masks = masks.repeat(self._num_heads, 1, 1)  # (Bh (N+E) HW)\n",
        "\n",
        "        # image_encodings = self._patch_encondings(images.mask)\n",
        "\n",
        "        nodes = graph.nodes(True)  # (B N D)\n",
        "        edges = graph.edges(True)  # (B E D)\n",
        "        x = torch.cat((nodes, edges), dim=1)  # (B (N+E) D)\n",
        "\n",
        "        for block in self._layers:\n",
        "            x = block(\n",
        "                x,\n",
        "                flattened_images.tensor,\n",
        "                masks,\n",
        "            )\n",
        "\n",
        "        nodes = x[:, : nodes.shape[1]]  # (B N D)\n",
        "        edges = x[:, nodes.shape[1] :]  # (B E D)\n",
        "\n",
        "        return graph.new_like(nodes, edges)\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UhFn-LdhqkQL"
      },
      "source": [
        "### Training ERP-A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ugIP8gwklXX"
      },
      "outputs": [],
      "source": [
        "from visgator.engines.trainer import Config as TrainerConfig\n",
        "from visgator.engines.trainer import Trainer\n",
        "from typing import Any\n",
        "\n",
        "train_config = TrainerConfig.from_dict(cfg)\n",
        "trainer: Trainer[Any] = Trainer(train_config)\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWp1zI_5cSVj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ORllucLOXRb8",
        "D85DeiT4nXq4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
